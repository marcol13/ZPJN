{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inżynieria lingwistyczna\n",
    "Ten notebook jest oceniany półautomatycznie. Nie twórz ani nie usuwaj komórek - struktura notebooka musi zostać zachowana. Odpowiedź wypełnij tam gdzie jest na to wskazane miejsce - odpowiedzi w innych miejscach nie będą sprawdzane (nie są widoczne dla sprawdzającego w systemie).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 1 - tokenizacja (12 pkt)\n",
    "\n",
    "Jedną z nowoczesnych technik tokenizacji jest BPE - byte-pair encoding [1]. Technika ta polega na podzielenie słów na częste podsłowa (a'la morfemy?). W przeciwieństwie do podejść lingwistycznych, wymagających zwykle ręcznie napisanych reguł tworzenia morfemów czy nawet słowników lematów, BPE znajduje je heurystycznie poprzez wyznaczenie najczęstszych przylegających do siebie sekwencji znaków.\n",
    "\n",
    "Algorytm przebiega w następujących krokach.\n",
    "1. Podziel wszystkie słowa na symbole (początkowo pojedyncze znaki)\n",
    "2. Wyznacz najczęściej występującą obok siebie parę symboli \n",
    "3. Stwórz nowy symbol będący konkatenacją dwóch najczęstszych symboli.\n",
    "\n",
    "Uwaga 1: każde słowo zakończone jest specjalnym symbolem końca wyrazu.\n",
    "\n",
    "Uwaga 2: tworzenie nowego symbolu nie powoduje usuniecie starego tj. zawsze jednym z możliwych symboli jest pojedynczy znak, ale jeśli można to stosujemy symbol dłuższy.\n",
    "\n",
    "Przykład: korpus w którym występuje ,,ala'' 5 razy i ,,mama 10 razy''\n",
    "1. Dzielimy słowa na symbole ,,a l a END'' ,,m a m a END''  gdzie END jest symbolem końca wyrazu.\n",
    "2. Najczęstsza para obok siebie to ,,m a'' (20 razy)\n",
    "3. Nowy symbol ,,ma''\n",
    "4. Nowy podział ,,a l a END'' ,,ma ma END''\n",
    "5. Najczęstsza para ,,ma ma'' (10 razy)\n",
    "6. Nowy symbol ,,mama''\n",
    "7. Nowy podział ,,a l a END'' ,,mama END''\n",
    "8. itd.\n",
    "\n",
    "W pliku ,,brown_clusters.tsv'' pierwsza kolumna to identyfikator skupienia (nie używamy w tym zadaniu), druga kolumna to wyrazy, a trzecia to ich liczności w pewnym korpusie tweetów. Zaimplementuj technikę BPE i przetesuj ją na tych słowach.\n",
    "\n",
    "Parametrem algorytmu BPE jest `number_of_iterations` czyli liczba iteracji (łączeń symboli). Dodatkowo implementacja powinna mieć parametr `verbose`, którego wartość `True` powinna skutkować wypisywaniem na konsolę wykonywanych operacji (tj. łączeń).\n",
    "\n",
    "[1] Sennrich, R., Haddow, B., and Birch, A. (2016). Neural machine translation of rare words with subword units. In ACL 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfc05e34c3acef10de92eec00b46791d",
     "grade": false,
     "grade_id": "cell-93d78a28d4e25cbc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 1830.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: ma\n",
      "join: mama\n",
      "join: mamaEND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a l a END', 'mamaEND']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict \n",
    "\n",
    "brown_df = pd.read_csv('brown_clusters.tsv', sep='\\t', header=0, names=['cluster', 'word', 'count'])\n",
    "number_of_iterations = 10\n",
    "\n",
    "def join_tuples(df, tup):\n",
    "     for _, row in df.iterrows():\n",
    "            count_join = 0\n",
    "            for index in range(len(row[\"word\"]) - 1):\n",
    "                if index + count_join > len(row[\"word\"]) - 1:\n",
    "                    break\n",
    "                temp_tuple = row[\"word\"][index] + row[\"word\"][index + 1]\n",
    "                if temp_tuple == tup:\n",
    "                    row[\"word\"][index] = tup\n",
    "                    del row[\"word\"][index + 1]\n",
    "                    count_join += 1\n",
    "     return df\n",
    "\n",
    "def preform_bpe(brown_df, number_of_iterations, verbose = False):\n",
    "    \"\"\"\n",
    "    Funckcja przyjmuje ramkę w formacie analogicznym do obiektu brown_df (wczytany wyżej)\n",
    "     oraz liczbę iteracji.\n",
    "    Wyjściem funkcji powinna być lista słów z poszczególnymi tokenami/symbolami oddzielonymi spacją.\n",
    "    Za znak końca wyrazu przyjmij END. \n",
    "    \"\"\"\n",
    "    # alph = [*string.ascii_lowercase, \"END\"]\n",
    "    df = brown_df.drop(columns=[\"cluster\"])\n",
    "    df[\"word\"] = df[\"word\"].apply(lambda x: [*str(x), \"END\"])\n",
    "\n",
    "    for _ in tqdm(range(number_of_iterations)):\n",
    "        max_join = \"\"\n",
    "        count_dict = defaultdict(lambda: 0)\n",
    "\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            for index in range(len(row[\"word\"]) - 1):\n",
    "                join = row[\"word\"][index] + row[\"word\"][index + 1]\n",
    "                count_dict[join] += row[\"count\"]\n",
    "\n",
    "        max_join = max(count_dict, key=count_dict.get)\n",
    "        count_dict[max_join] = 0\n",
    "        df = join_tuples(df, max_join)\n",
    "        if(verbose):\n",
    "            print(f\"join: {max_join}\")\n",
    "    \n",
    "    return df[\"word\"].apply(lambda x: \" \".join(x)).tolist()\n",
    "\n",
    "data = {'cluster': range(2), 'word':['ala', 'mama'], 'count': [5,10]}\n",
    "df = pd.DataFrame (data, columns = ['cluster', 'word', 'count'])\n",
    "preform_bpe(df, 3, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test implementacji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dfff70f711bf389f0f1cd969e7c3a413",
     "grade": true,
     "grade_id": "cell-7e952fa8dcd136fe",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 967.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from nose.tools import assert_list_equal\n",
    "data = {'cluster': range(2), 'word':['ala', 'mama'], 'count': [5,10]}\n",
    "df = pd.DataFrame (data, columns = ['cluster', 'word', 'count'])\n",
    "vocab = preform_bpe(df, 1)\n",
    "assert_list_equal(vocab, ['a l a END', 'ma ma END'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spraw aby Twoja implementacja wypisywała kolejne łączone ze sobą symbole (parametr `verbose`) i uruchom Twoją funkcję na np. 50 iteracji, obserwując jakie tokeny są tworzone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:35<28:36, 35.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: eEND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [01:08<27:30, 34.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: tEND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [01:42<26:30, 33.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: sEND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [02:14<25:37, 33.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [02:47<24:47, 33.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [03:19<24:06, 32.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: dEND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [03:52<23:28, 32.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: yEND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [04:24<22:46, 32.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: .END\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [04:56<22:07, 32.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: oEND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [05:28<21:29, 32.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: rEND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [06:00<20:53, 32.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: an\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [06:32<20:20, 32.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: >END\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [07:04<19:51, 32.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [07:36<19:18, 32.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: ou\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [08:08<18:41, 32.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: gEND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [08:40<18:06, 31.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: aEND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [09:11<17:30, 31.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: lEND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [09:43<16:56, 31.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: ingEND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [10:14<16:19, 31.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: <@\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [17:43<1:18:23, 156.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: <@M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [20:03<1:13:21, 151.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: <@ME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [22:29<1:09:59, 149.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: <@MEN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [24:55<1:06:55, 148.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: <@MENT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [27:22<1:04:16, 148.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: <@MENTI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [29:46<1:01:13, 146.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: <@MENTIO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [32:04<57:44, 144.37s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: <@MENTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [34:25<55:00, 143.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: <@MENTION>END\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [36:49<52:39, 143.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: re\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [39:12<50:11, 143.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: iEND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [41:35<47:42, 143.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: theEND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [43:57<45:14, 142.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [46:19<42:47, 142.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: om\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [48:38<40:07, 141.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: toEND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [51:00<37:45, 141.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: ,END\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [53:22<35:24, 141.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: !END\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [55:43<33:03, 141.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: er\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [58:04<30:37, 141.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: ha\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [1:00:24<28:10, 140.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: erEND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [1:02:44<25:46, 140.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [1:05:03<23:22, 140.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: :END\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [1:07:23<21:01, 140.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: you\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [1:09:43<18:41, 140.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: ar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [1:12:02<16:17, 139.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: al\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [1:14:20<13:55, 139.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: or\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [1:16:32<11:25, 137.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: ow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [1:17:10<07:08, 107.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: ..END\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [1:17:43<04:14, 84.92s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: st\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [1:18:15<02:18, 69.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: kEND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [1:18:48<00:58, 58.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: isEND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [1:19:20<00:00, 95.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join: fEND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\\\ iEND',\n",
       " '/ i / END',\n",
       " 't o d a y - iEND',\n",
       " 'n ow iEND',\n",
       " '# you e v erEND',\n",
       " 'i f in al l yEND',\n",
       " '「 iEND',\n",
       " '- i - END',\n",
       " 'in e v aEND',\n",
       " '» iEND',\n",
       " 'w ha t t a y aEND',\n",
       " 'i i i i i i i i i iEND',\n",
       " '\\ue6d1 END',\n",
       " 'i k in d aEND',\n",
       " 'l o l - iEND',\n",
       " 'i a c t u al l yEND',\n",
       " 'w a d d y aEND',\n",
       " '# a s l on g a s you END',\n",
       " 'd o you END',\n",
       " '\\u200e \\u200b iEND',\n",
       " 'i ̇ END',\n",
       " 'ï END',\n",
       " '# l o l a t g i r l s w h oEND',\n",
       " '# r t i f you END',\n",
       " 'i j s tEND',\n",
       " '« iEND',\n",
       " '• iEND',\n",
       " 'w h o d aEND',\n",
       " 'w ha d y aEND',\n",
       " ') iEND',\n",
       " '+ iEND',\n",
       " '# you r f a c e m a k e s m eEND',\n",
       " 'i i i i i i i iEND',\n",
       " '` iEND',\n",
       " 'i i i i i i iEND',\n",
       " 'i al re a d yEND',\n",
       " '_ iEND',\n",
       " '# you m a k e m eEND',\n",
       " '* iEND',\n",
       " '| iEND',\n",
       " '# u r b o y f r i en d e v erEND',\n",
       " 'w h en iEND',\n",
       " 'ι END',\n",
       " \"d on ' t c h aEND\",\n",
       " \"w h o ' d aEND\",\n",
       " 'd you END',\n",
       " 'w ha d d a y aEND',\n",
       " 'i on l yEND',\n",
       " 'i j u s sEND',\n",
       " 'i al w a y sEND',\n",
       " 'i i i i iEND',\n",
       " 'd on c h aEND',\n",
       " '( iEND',\n",
       " \"d ' y aEND\",\n",
       " 'ı END',\n",
       " '# u e v erEND',\n",
       " 'in e v erEND',\n",
       " 'i - iEND',\n",
       " 'i j u sEND',\n",
       " '/ / iEND',\n",
       " 'i st i l lEND',\n",
       " 'w ha d d y aEND',\n",
       " \"d ' you END\",\n",
       " 'i re al l yEND',\n",
       " 'd on t c h aEND',\n",
       " 'i j u s tEND',\n",
       " 'iEND',\n",
       " '- iEND',\n",
       " 'i you END',\n",
       " '# in n ow a y s ha p e or f or m END',\n",
       " '( you END',\n",
       " '/ / w eEND',\n",
       " '/ / u END',\n",
       " '# m en m ar r y w om en th a tEND',\n",
       " '/ w eEND',\n",
       " 's e l f - e d u c a t i on END',\n",
       " '# re al g r an d m a sEND',\n",
       " '/ you END',\n",
       " '# s h ou t ou t t o g i r l s w h oEND',\n",
       " '# b o y s w h oEND',\n",
       " 'i / w eEND',\n",
       " '# s h ou t ou t t o th e g u y s th a tEND',\n",
       " '/ / you END',\n",
       " '# i l o v e p e o p l e th a tEND',\n",
       " '# n o t al l b l a c k p e o p l eEND',\n",
       " '# i c an t st an d p e o p l e th a tEND',\n",
       " '# s h ou t ou t t o th e g i r l s th a tEND',\n",
       " '- th e yEND',\n",
       " '- w eEND',\n",
       " '# h ow m an y p e o p l eEND',\n",
       " '- you END',\n",
       " 'w eEND',\n",
       " '# a q u ar i an sEND',\n",
       " 't th e yEND',\n",
       " 'th w yEND',\n",
       " 'g u i l d en st er n END',\n",
       " \"d ' u END\",\n",
       " '# i ha t e m al e s w h oEND',\n",
       " 't e h yEND',\n",
       " 'th r yEND',\n",
       " 'i f you END',\n",
       " '# h ou s e h i p p o sEND',\n",
       " 'th e u END',\n",
       " 'th e e yEND',\n",
       " '# i ha t e f e m al e s w h oEND',\n",
       " 'th e y yEND',\n",
       " 'th e yEND',\n",
       " 'v i o l e t sEND',\n",
       " 'e h oEND',\n",
       " 'w h o ’ dEND',\n",
       " 'w h o t fEND',\n",
       " 'w h o ’ v eEND',\n",
       " 'w h o dEND',\n",
       " '< U R L - re al . c om >END',\n",
       " '# i l i k e p e o p l e w h oEND',\n",
       " '- w h oEND',\n",
       " 'w h 0 END',\n",
       " 'w h u END',\n",
       " 'w h oEND',\n",
       " \"w h o ' v eEND\",\n",
       " 's s h eEND',\n",
       " 's er - u e b er w a c h erEND',\n",
       " 's h h eEND',\n",
       " 't e st a st er i s kEND',\n",
       " '# m y d u m b a s sEND',\n",
       " 's j eEND',\n",
       " 't a c h om a st erEND',\n",
       " 'i al m o s tEND',\n",
       " 'i d on eEND',\n",
       " '# w ha t i f iEND',\n",
       " 'h e / s h e / i tEND',\n",
       " '$ h eEND',\n",
       " '# w ha t i f g o dEND',\n",
       " '# i h e ar d c h u c k n or r isEND',\n",
       " '# f m h 2 0 1 1 END',\n",
       " '# i h e ar d b ow w ow END',\n",
       " 'b l d _ 6 0 0 _ k w h END',\n",
       " 'b l d _ 6 5 0 _ k w h END',\n",
       " 's h e e eEND',\n",
       " '# f m 2 0 1 1 END',\n",
       " '- s h eEND',\n",
       " '- h eEND',\n",
       " 's / h eEND',\n",
       " 's h e / h eEND',\n",
       " '- i tEND',\n",
       " 's h e eEND',\n",
       " 'h e / s h eEND',\n",
       " 'h eEND',\n",
       " 's h eEND',\n",
       " '< U R L - i . v e >END',\n",
       " \"l ' v eEND\",\n",
       " 'th e y v END',\n",
       " \"you \\\\ ' v eEND\",\n",
       " \"i ' b eEND\",\n",
       " '< U R L - i h e ar t m o v i e s . or g >END',\n",
       " 'i w ou l d aEND',\n",
       " 'w e ` v eEND',\n",
       " \"i ha v en ' tEND\",\n",
       " '# i ha v en e v erEND',\n",
       " \"a : i ' v eEND\",\n",
       " 'w e v END',\n",
       " 'w e ´ v eEND',\n",
       " \"y u ' v eEND\",\n",
       " 'u ’ v eEND',\n",
       " \"- i ' v eEND\",\n",
       " \"th e re ' v eEND\",\n",
       " \"i ' d aEND\",\n",
       " '< U R L - v o om a x er . c om >END',\n",
       " \"th a t ' v eEND\",\n",
       " \"w e ' v END\",\n",
       " 'you ´ v eEND',\n",
       " 'i v e eEND',\n",
       " \"i ' d ' v eEND\",\n",
       " 'you ` v eEND',\n",
       " \"i \\\\ ' v eEND\",\n",
       " 'you v END',\n",
       " \"you ' v END\",\n",
       " '# n e v er ha v e i e v erEND',\n",
       " \"u ' v END\",\n",
       " '< U R L - n a u g h t y d o g . c om >END',\n",
       " 'i ha v en tEND',\n",
       " \"i v ' eEND\",\n",
       " 'th e y ’ v eEND',\n",
       " '# ha v e you e v erEND',\n",
       " 'i ´ v eEND',\n",
       " 'i ` v eEND',\n",
       " '# ha v e u e v erEND',\n",
       " 'th e y v eEND',\n",
       " \"i ' v END\",\n",
       " '0 . 0 0 % END',\n",
       " 'w e v eEND',\n",
       " 'u v eEND',\n",
       " 'w e ’ v eEND',\n",
       " \"u ' v eEND\",\n",
       " 'you ’ v eEND',\n",
       " 'you v eEND',\n",
       " 'i ’ v eEND',\n",
       " \"th e y ' v eEND\",\n",
       " \"i ' v eEND\",\n",
       " 'i v eEND',\n",
       " \"you ' v eEND\",\n",
       " \"w e ' v eEND\",\n",
       " 'n o o o o o o tEND',\n",
       " 'n o t t t t t t tEND',\n",
       " 'n o h tEND',\n",
       " 'you ha v eEND',\n",
       " '/ n o t / END',\n",
       " 'n o i tEND',\n",
       " '- n o t - END',\n",
       " 'n o t t t t t tEND',\n",
       " 'n o o o o o tEND',\n",
       " \"n ' tEND\",\n",
       " 'n n o tEND',\n",
       " 'n a h tEND',\n",
       " '_ n o t _ END',\n",
       " 'd e s er v e d l yEND',\n",
       " 'n o t t t t tEND',\n",
       " 'n o o o o tEND',\n",
       " 'n o t - END',\n",
       " 'n o o o tEND',\n",
       " 'n o o tEND',\n",
       " 'n toEND',\n",
       " 'n o t t t tEND',\n",
       " 'n o t t tEND',\n",
       " 'r i g h t f u l l yEND',\n",
       " 'n a w tEND',\n",
       " 'n 0 tEND',\n",
       " 'n o t tEND',\n",
       " 'n o tEND',\n",
       " 'n tEND',\n",
       " 'g o t t n END',\n",
       " 'b 3 3 n END',\n",
       " 'b e e e e e en END',\n",
       " 'g o t t on END',\n",
       " 's u c c e s s f u l yEND',\n",
       " 'b e e e e en END',\n",
       " 'b e en n END',\n",
       " 'u n d er g on eEND',\n",
       " 'b e e e en END',\n",
       " 'b e e en END',\n",
       " 'b e en END',\n",
       " 'g o t t en END',\n",
       " 'j u x tEND',\n",
       " '/ / j u s tEND',\n",
       " 'j u st t t t tEND',\n",
       " 'j x tEND',\n",
       " '# s p or c l eEND',\n",
       " 'j st tEND',\n",
       " 'j u r tEND',\n",
       " 'j y sEND',\n",
       " '/ j u s tEND',\n",
       " '- j u sEND',\n",
       " 'j y s tEND',\n",
       " 'd d e u b e lEND',\n",
       " 'j u st t t tEND',\n",
       " 'j u s s s s tEND',\n",
       " 'j u $ tEND',\n",
       " 'j u u sEND',\n",
       " 'j s s tEND',\n",
       " 'j u u u u u s tEND',\n",
       " 'k u s tEND',\n",
       " 'j h u sEND',\n",
       " 'j u s s s sEND',\n",
       " 'j h u s sEND',\n",
       " 'j u st sEND',\n",
       " 'j u s s s tEND',\n",
       " 'j u s x END',\n",
       " 'j u x x END',\n",
       " 'j z tEND',\n",
       " 'j u z z END',\n",
       " 'j u d tEND',\n",
       " '< U R L - w o o . l y >END',\n",
       " 'j u h sEND',\n",
       " 'j u u u u s tEND',\n",
       " 'j j u s tEND',\n",
       " 'j u z tEND',\n",
       " 'j u st t tEND',\n",
       " 'j u st e dEND',\n",
       " 'j u s rEND',\n",
       " 'j u u u s tEND',\n",
       " 'j u t sEND',\n",
       " 'j u s yEND',\n",
       " 'j u u s tEND',\n",
       " 'j u a tEND',\n",
       " 'j u s z END',\n",
       " '# j u s tEND',\n",
       " 'j s sEND',\n",
       " 'j u s s sEND',\n",
       " '< U R L - b u y t t er . c om >END',\n",
       " 'j u s s tEND',\n",
       " 'j z END',\n",
       " '- j u s tEND',\n",
       " '# d on t a c t l i k e you n e v erEND',\n",
       " 'j u tEND',\n",
       " 'j u st tEND',\n",
       " 'j u x END',\n",
       " 'j s u tEND',\n",
       " 'j u z END',\n",
       " 'j s tEND',\n",
       " 'j u s sEND',\n",
       " 'j u s tEND',\n",
       " 'j u sEND',\n",
       " \"a in \\\\ ' tEND\",\n",
       " 'a in ´ tEND',\n",
       " \"a ' in tEND\",\n",
       " 'a it n END',\n",
       " 'a in yEND',\n",
       " 'w u s z END',\n",
       " 'a y n tEND',\n",
       " 'a in n tEND',\n",
       " 'i an tEND',\n",
       " \"an ' tEND\",\n",
       " 'a in eEND',\n",
       " 'a in ` tEND',\n",
       " 'a in n END',\n",
       " 'a in t tEND',\n",
       " 'a i in tEND',\n",
       " 'i a in tEND',\n",
       " 'a in ’ tEND',\n",
       " 'an i tEND',\n",
       " 'a in END',\n",
       " 'a in tEND',\n",
       " \"a in ' tEND\",\n",
       " 's h ou d aEND',\n",
       " 's h ou l d n aEND',\n",
       " 's h ou l aEND',\n",
       " \"w ou l d n ' t ' v eEND\",\n",
       " \"s h ou l d n ' t ' v eEND\",\n",
       " 's h ou l d v END',\n",
       " \"s h u d ' v eEND\",\n",
       " 'h an tEND',\n",
       " \"ha v ' n tEND\",\n",
       " 'c l d aEND',\n",
       " 'w l d v eEND',\n",
       " \"s h ou l d ' aEND\",\n",
       " 'w ou l d a aEND',\n",
       " 's h ou l d d aEND',\n",
       " 'w u l d v eEND',\n",
       " \"w u d ' v eEND\",\n",
       " 'w ou l d d aEND',\n",
       " 's h u l d v eEND',\n",
       " 's h ou l d a aEND',\n",
       " \"ha v e ' tEND\",\n",
       " 'c ou l d ’ v eEND',\n",
       " 'ha v en t tEND',\n",
       " 's h l d v eEND',\n",
       " 'c u d v eEND',\n",
       " \"m a y ' v eEND\",\n",
       " \"h v n ' tEND\",\n",
       " 'w ou l d ’ v eEND',\n",
       " 'a v n tEND',\n",
       " 'w l d aEND',\n",
       " 's h ou l d ’ v eEND',\n",
       " 'c u l d aEND',\n",
       " 'ha v en ´ tEND',\n",
       " 's h l d aEND',\n",
       " 'm i g h t v eEND',\n",
       " 'ha v en ` tEND',\n",
       " 'ha d n ’ tEND',\n",
       " '# g l o c al u r b an END',\n",
       " 'h v en tEND',\n",
       " 's h u d v eEND',\n",
       " 'w u d v eEND',\n",
       " \"ha v e ' n tEND\",\n",
       " 'c u d d aEND',\n",
       " 'm i g h t aEND',\n",
       " 'w u l d aEND',\n",
       " 's h u l d aEND',\n",
       " 'w u d d aEND',\n",
       " 's h u d d aEND',\n",
       " 'w u d aEND',\n",
       " 's h u d aEND',\n",
       " 'm u st v eEND',\n",
       " 'h v n tEND',\n",
       " \"m i g h t ' v eEND\",\n",
       " 'ha d n tEND',\n",
       " \"ha v n ' tEND\",\n",
       " 'ha v en ’ tEND',\n",
       " 'c ou l d v eEND',\n",
       " 'm u st aEND',\n",
       " \"m u st ' v eEND\",\n",
       " 'w ou l d v eEND',\n",
       " 's h ou l d v eEND',\n",
       " 'ha v n tEND',\n",
       " 'c ou l d aEND',\n",
       " \"c ou l d ' v eEND\",\n",
       " 'w ou l d aEND',\n",
       " \"ha d n ' tEND\",\n",
       " \"s h ou l d ' v eEND\",\n",
       " \"w ou l d ' v eEND\",\n",
       " 's h ou l d aEND',\n",
       " \"ha v en ' tEND\",\n",
       " 'ha v en tEND',\n",
       " 'n e v v aEND',\n",
       " 'n e e e e v erEND',\n",
       " 'n e v e tEND',\n",
       " 'n e e e v erEND',\n",
       " 'en v erEND',\n",
       " 'n er v erEND',\n",
       " 'n e e v erEND',\n",
       " 'n e v a a aEND',\n",
       " 'b e v erEND',\n",
       " '# in e v erEND',\n",
       " 'g l a d yEND',\n",
       " 'n e v e erEND',\n",
       " '- n e v erEND',\n",
       " \"n e ' erEND\",\n",
       " 'l e t c h aEND',\n",
       " 'l e t c h u END',\n",
       " 'n e v er r r rEND',\n",
       " 'n v aEND',\n",
       " 'n e v a h END',\n",
       " 'n e v a aEND',\n",
       " 'n e v er r rEND',\n",
       " 'n v erEND',\n",
       " 'n e v er rEND',\n",
       " '# n e v erEND',\n",
       " 'n e v rEND',\n",
       " 'g l a d l yEND',\n",
       " 'n v rEND',\n",
       " 'n e v erEND',\n",
       " 'n e v aEND',\n",
       " 'e v u rEND',\n",
       " 'e v a a a a aEND',\n",
       " 'e v e aEND',\n",
       " 'e v e e e erEND',\n",
       " 'e v er r r r r r r r rEND',\n",
       " 'e v er r r r r r r rEND',\n",
       " 'e v e e erEND',\n",
       " 'e v a a a aEND',\n",
       " 'e v e erEND',\n",
       " 'n e v a rEND',\n",
       " 'e v er r r r r r rEND',\n",
       " 'e v a a aEND',\n",
       " 'e v a aEND',\n",
       " 'e v er r r r r rEND',\n",
       " 'e v er r r r rEND',\n",
       " 'e v a h END',\n",
       " 'e v er r r rEND',\n",
       " 'e v er rEND',\n",
       " 'e v er r rEND',\n",
       " 'e v rEND',\n",
       " 'e v a rEND',\n",
       " 'e v aEND',\n",
       " 'e v erEND',\n",
       " 'on l eEND',\n",
       " 'in l yEND',\n",
       " 'on l e eEND',\n",
       " 'on l u END',\n",
       " 'on y lEND',\n",
       " 'on l l yEND',\n",
       " 'on l tEND',\n",
       " 'on l y y yEND',\n",
       " 'o l n yEND',\n",
       " '- on l yEND',\n",
       " '0 n l yEND',\n",
       " 'on l i iEND',\n",
       " 'on yEND',\n",
       " 'on l y yEND',\n",
       " 'on l yEND',\n",
       " 'on l iEND',\n",
       " 'g e t 2 END',\n",
       " 'n e c c e s ar i l yEND',\n",
       " 'n e c c e s s ar i l yEND',\n",
       " 'e e e m END',\n",
       " 'e v er n END',\n",
       " 'n e v en END',\n",
       " 'l e t e m END',\n",
       " 'e v en n n END',\n",
       " 'e v e b END',\n",
       " 'e e en END',\n",
       " 'e v e m END',\n",
       " 'e v e en END',\n",
       " '< U R L - g t p 1 2 3 . c om >END',\n",
       " '- e v en END',\n",
       " \"1 0 x ' sEND\",\n",
       " \"m a k e ' e m END\",\n",
       " \"l e t ' e m END\",\n",
       " 'e v en n END',\n",
       " 'e e m END',\n",
       " 'e v n END',\n",
       " 'e v en END',\n",
       " 'n e c e s s ar i l yEND',\n",
       " 're e e e e e e al l yEND',\n",
       " 're a a al yEND',\n",
       " 're al l l l l l l l l l yEND',\n",
       " 'r l iEND',\n",
       " 're al l i eEND',\n",
       " 're al l l l l y y yEND',\n",
       " 're e l iEND',\n",
       " 'r 3 al l yEND',\n",
       " 're al l l l y yEND',\n",
       " 're al l y - re al l yEND',\n",
       " 're l al yEND',\n",
       " 're al l l l y y y yEND',\n",
       " 'r i l l iEND',\n",
       " 're al l y re al l y re al l yEND',\n",
       " '- re al l y - END',\n",
       " 're al l y y y y y yEND',\n",
       " 're a al l y yEND',\n",
       " 'e al l yEND',\n",
       " 're e e a a al l yEND',\n",
       " 're al l y re al l yEND',\n",
       " 're e a al l yEND',\n",
       " 'r re al l yEND',\n",
       " 're a a a al l l yEND',\n",
       " 're al l u END',\n",
       " 're a a a a a al l yEND',\n",
       " '/ re al l y / END',\n",
       " 're al y yEND',\n",
       " 're a al l l yEND',\n",
       " 're al l l l l l l l l yEND',\n",
       " 're al l l l y y yEND',\n",
       " 're a a al l l yEND',\n",
       " 'w e al l yEND',\n",
       " 're e e e e e al l yEND',\n",
       " 're al l l y y y yEND',\n",
       " 're l l iEND',\n",
       " 'g en u in l yEND',\n",
       " 're al l tEND',\n",
       " 're al l i iEND',\n",
       " 're al l l y yEND',\n",
       " 're a al yEND',\n",
       " 're al l l l l l l l yEND',\n",
       " '_ re al l y _ END',\n",
       " 're al l y y y y yEND',\n",
       " 're al l y 2 END',\n",
       " 's h o l eEND',\n",
       " 're a a a a al l yEND',\n",
       " 're e l yEND',\n",
       " 're l l eEND',\n",
       " 're al l l y y yEND',\n",
       " 's h o lEND',\n",
       " 're e al l yEND',\n",
       " 're e e e e al l yEND',\n",
       " 're al l l l l l l yEND',\n",
       " 'r i l l yEND',\n",
       " 're al l y y y yEND',\n",
       " 're a a a al l yEND',\n",
       " 're a a al l yEND',\n",
       " 'r i l iEND',\n",
       " 're e e al l yEND',\n",
       " 're a al l yEND',\n",
       " 're al l l l l l yEND',\n",
       " 're e e e al l yEND',\n",
       " 're al l y y yEND',\n",
       " 'r i l yEND',\n",
       " 's h o l lEND',\n",
       " 're al iEND',\n",
       " 're l iEND',\n",
       " 're al l l l l yEND',\n",
       " 're l l yEND',\n",
       " 're al l iEND',\n",
       " 're l eEND',\n",
       " 're al l y yEND',\n",
       " 're al l l l yEND',\n",
       " 're al l l yEND',\n",
       " 'r l l yEND',\n",
       " 'g en u in e l yEND',\n",
       " 're al yEND',\n",
       " 're al l yEND',\n",
       " 'r l yEND',\n",
       " 'al re d a yEND',\n",
       " 'al re a y dEND',\n",
       " 'f in al iEND',\n",
       " 'o f f i s h END',\n",
       " 'al r a d yEND',\n",
       " 'w o o d aEND',\n",
       " 'o re d iEND',\n",
       " 'al re a a d yEND',\n",
       " 'al re a d y y y y yEND',\n",
       " 'al re a d d yEND',\n",
       " 'al e a d yEND',\n",
       " 'al re a yEND',\n",
       " 's u c e s s f u l l yEND',\n",
       " 'al re d iEND',\n",
       " 'a re a d yEND',\n",
       " 'al re a d iEND',\n",
       " 'al re a d i iEND',\n",
       " 'al re a dEND',\n",
       " 'al re a d y y yEND',\n",
       " 'a w re a d yEND',\n",
       " 'al r dEND',\n",
       " 'c u d aEND',\n",
       " 'al re d yEND',\n",
       " 'al l re a d yEND',\n",
       " 'al re a d y yEND',\n",
       " 'al r d yEND',\n",
       " 'p re v i ou s l yEND',\n",
       " 'al re a d yEND',\n",
       " 're c en t l yEND',\n",
       " '- al m o s tEND',\n",
       " 'n e al yEND',\n",
       " 'n e ar l l yEND',\n",
       " 'al m 0 s tEND',\n",
       " 'al om o s tEND',\n",
       " 'al om s tEND',\n",
       " 'al m o st tEND',\n",
       " 'al m s o tEND',\n",
       " 'a m o s tEND',\n",
       " 'al l m o s tEND',\n",
       " 'al m s tEND',\n",
       " 'a v er a g ingEND',\n",
       " 'r ou g h l yEND',\n",
       " 'v i r t u al l yEND',\n",
       " 'a p p r o x i m a t e l yEND',\n",
       " 'p r a c t i c al l yEND',\n",
       " 'al m o s tEND',\n",
       " 'n e ar l yEND',\n",
       " 's i b b yEND',\n",
       " 'c u r re n t yEND',\n",
       " 'o f f i c a i l l yEND',\n",
       " 'o f f c i al l yEND',\n",
       " '# b g g p l a yEND',\n",
       " 'c u re n t l yEND',\n",
       " 'b u s i l yEND',\n",
       " 'o f f i c al yEND',\n",
       " 'o f i c i al l yEND',\n",
       " 'c or d i al l yEND',\n",
       " '< U R L - l i st en . g h e t t or a d i o . f m >END',\n",
       " '< U R L - k a i s e re g g . c h >END',\n",
       " 'h u c k l e b er r i e sEND',\n",
       " 'h e i s eEND',\n",
       " '# re a d c a s tEND',\n",
       " 'p re s en t l yEND',\n",
       " 'o f f i c i al yEND',\n",
       " '< U R L - g o . n i k e . c om >END',\n",
       " 'o f f i c al l yEND',\n",
       " 're p or t e d l yEND',\n",
       " 'o f f i c i al l yEND',\n",
       " 'c u r re n t l yEND',\n",
       " 'f in al l l y yEND',\n",
       " 'f in al l iEND',\n",
       " 'f in al l l y y yEND',\n",
       " '# o f f i c i al l yEND',\n",
       " 'f i i i in al l yEND',\n",
       " 'f n al l yEND',\n",
       " 'f in al l y y y y yEND',\n",
       " 'f i in al l yEND',\n",
       " '- f in al l yEND',\n",
       " 'b er l yEND',\n",
       " 'f in al l l l l l yEND',\n",
       " '# th in g s i d i d o v er th e s u m m erEND',\n",
       " '< U R L - m a p m y f it n e s s . c om >END',\n",
       " 'f in al l l l l yEND',\n",
       " 'f in i al l yEND',\n",
       " 's n a c k f e e dEND',\n",
       " 'f in al l y y y yEND',\n",
       " 'f in al l l l yEND',\n",
       " 'f i an l l yEND',\n",
       " '< U R L - m a p m y r i d e . c om >END',\n",
       " 'f in al l y y yEND',\n",
       " 's u c c e s f u l l yEND',\n",
       " 'f in al l y yEND',\n",
       " '# m y f it n e s s p a lEND',\n",
       " 'f in n al l yEND',\n",
       " '< U R L - m a p m y r u n . c om >END',\n",
       " 'f in al l l yEND',\n",
       " 're l u c t an t l yEND',\n",
       " 'f in n al yEND',\n",
       " '< U R L - c o or d . in f o >END',\n",
       " 'f in al yEND',\n",
       " 'f in al l yEND',\n",
       " 's u c c e s s f u l l yEND',\n",
       " 'k n ow e s tEND',\n",
       " 'r a th rEND',\n",
       " 'c an s tEND',\n",
       " '< U R L - s u p er m ar k e t . c om >END',\n",
       " 'r a th aEND',\n",
       " 'r a th erEND',\n",
       " 's ha l tEND',\n",
       " \"d on t ' c h aEND\",\n",
       " 'i i on END',\n",
       " 'i m m oEND',\n",
       " '4 + 4 END',\n",
       " 'i i b END',\n",
       " 'i d on t tEND',\n",
       " 'b r in j a lEND',\n",
       " 'i d on END',\n",
       " 'th e l lEND',\n",
       " '2 iEND',\n",
       " 'n u s tEND',\n",
       " 'a b t aEND',\n",
       " 'l e y sEND',\n",
       " 'm e + you END',\n",
       " 'l e m m aEND',\n",
       " 's h ou l d sEND',\n",
       " 'w e l l iEND',\n",
       " 'i / i iEND',\n",
       " 'd i d s tEND',\n",
       " '1 iEND',\n",
       " 'c ha r s e tEND',\n",
       " 'u l dEND',\n",
       " 'd / n END',\n",
       " 'í END',\n",
       " 'f . i . n . a . l . s .END',\n",
       " 'i ow n END',\n",
       " 'i i dEND',\n",
       " 'n on - v i r g in END',\n",
       " 's k y ha w kEND',\n",
       " 's k y l an eEND',\n",
       " 'c h _ t y p eEND',\n",
       " 'k en o tEND',\n",
       " 'd in n yEND',\n",
       " '# i d on tEND',\n",
       " '- i iEND',\n",
       " '# y a m a m a e v erEND',\n",
       " '2 0 1 0 / 0 7 END',\n",
       " '2 0 1 0 / 0 5 END',\n",
       " 'c . l . a . s . s .END',\n",
       " '& iEND',\n",
       " 'i f u END',\n",
       " 'f m tEND',\n",
       " 'th a t iEND',\n",
       " 'l e m iEND',\n",
       " '# m y g o al f or 2 0 1 2 END',\n",
       " 'y u dEND',\n",
       " 'e b u END',\n",
       " 'b o t t aEND',\n",
       " 'm on e y - b a c kEND',\n",
       " '1 9 tEND',\n",
       " 'i 8 END',\n",
       " '# on l y f a t p e o p l eEND',\n",
       " '# th in g s i a in t d on e y e tEND',\n",
       " 'd on t c h u END',\n",
       " 's g eEND',\n",
       " 'i f iEND',\n",
       " 'i l dEND',\n",
       " '# c on f u s in g th in g s g i r l s d oEND',\n",
       " '_ i _ END',\n",
       " 'm u z END',\n",
       " 'c an iEND',\n",
       " '# u r g i r l f r i en d e v erEND',\n",
       " 'i - i - iEND',\n",
       " '# on l y u g l y p e o p l eEND',\n",
       " '2 0 1 0 / 1 2 END',\n",
       " 'l e t t sEND',\n",
       " 'n e erEND',\n",
       " '2 0 1 0 / 1 0 END',\n",
       " '2 0 1 0 / 0 4 END',\n",
       " 'c y a aEND',\n",
       " 's on tEND',\n",
       " '2 0 1 0 / 0 8 END',\n",
       " 'd on n END',\n",
       " '2 0 1 0 / 0 6 END',\n",
       " 'a p t - g e tEND',\n",
       " 'u on END',\n",
       " 'd o an END',\n",
       " 'd o s tEND',\n",
       " '# on l y w h it e p e o p l eEND',\n",
       " 'i i i iEND',\n",
       " '# th in g s b l a c k p e o p l e d oEND',\n",
       " 'i d w END',\n",
       " '2 + 2 END',\n",
       " 'in eEND',\n",
       " 'ha s tEND',\n",
       " 'i d i d n tEND',\n",
       " '1 + 1 END',\n",
       " 'h e dEND',\n",
       " 'p r o v o k ingEND',\n",
       " 'i d n tEND',\n",
       " 'u lEND',\n",
       " 'u m aEND',\n",
       " 'w dEND',\n",
       " 'u dEND',\n",
       " 'i i iEND',\n",
       " 'l lEND',\n",
       " 'i v END',\n",
       " 'i on END',\n",
       " 'i iEND',\n",
       " 'i dEND',\n",
       " 'p r a c t i c al yEND',\n",
       " 'u s s u al l yEND',\n",
       " 'a c c i d en t i al l yEND',\n",
       " 's u d en l yEND',\n",
       " 't e ar f u l l yEND',\n",
       " 'u s u al l l yEND',\n",
       " 'n a i v e l yEND',\n",
       " 're f l e x i v e l yEND',\n",
       " 'o p t i on al l yEND',\n",
       " 'l i k e 2 END',\n",
       " 'c on t in u o s l yEND',\n",
       " 'u n w i s e l yEND',\n",
       " 'p r a c t i c l yEND',\n",
       " 'a t u al l yEND',\n",
       " 'a c t l yEND',\n",
       " 'p u r p o s l yEND',\n",
       " 's e c re t e l yEND',\n",
       " 'ha r d l e yEND',\n",
       " 'a u t om a t i c al yEND',\n",
       " 'c on c e i v a b l yEND',\n",
       " 'a b s en t m in d e d l yEND',\n",
       " 'a c t u al l iEND',\n",
       " 'l it r al l yEND',\n",
       " 's u b c on c i ou s l yEND',\n",
       " 'c on st an l yEND',\n",
       " 'l it er l yEND',\n",
       " 'd o in tEND',\n",
       " 'm a k e m END',\n",
       " 's u p p o s e l yEND',\n",
       " 'a c t u al l y yEND',\n",
       " 'a c t u l yEND',\n",
       " 'g o an n aEND',\n",
       " 'b a s c i al l yEND',\n",
       " 'p r a t i c al l yEND',\n",
       " 'i i v eEND',\n",
       " 's u p p o s a b l yEND',\n",
       " 'b a s i c al yEND',\n",
       " 'er r on e ou s l yEND',\n",
       " 'f l a t l yEND',\n",
       " 'c a s j END',\n",
       " 'l e g it l yEND',\n",
       " 'l it t er l yEND',\n",
       " 'm u s s yEND',\n",
       " 'or g in al l yEND',\n",
       " 'd on rEND',\n",
       " 'in a d v er t an t l yEND',\n",
       " 'a c t al l yEND',\n",
       " 'a c t u al iEND',\n",
       " 'u s u s al l yEND',\n",
       " 'a c c u al l yEND',\n",
       " 'in t u it i v e l yEND',\n",
       " 'a u t om a t i c l yEND',\n",
       " 'g r u d g in g l yEND',\n",
       " 'b e g r u d g in g l yEND',\n",
       " 's c ar c e l yEND',\n",
       " 'b ar l yEND',\n",
       " 'l it t er al yEND',\n",
       " 'b l a t en t l yEND',\n",
       " 'ha b it u al l yEND',\n",
       " 'or d in ar i l yEND',\n",
       " 'a f f e c t i on a t e l yEND',\n",
       " 's n e a k i l yEND',\n",
       " 'a c c i d en t al yEND',\n",
       " 'n or m al yEND',\n",
       " 's in g l e h an d e d l yEND',\n",
       " 'c om p u l s i v e l yEND',\n",
       " 's u b l i m in al l yEND',\n",
       " 'in v o l u n t ar i l yEND',\n",
       " 'a c t u al l l yEND',\n",
       " 'g in eEND',\n",
       " 'd e f in it i v e l yEND',\n",
       " 'u s u al yEND',\n",
       " 'l it er al yEND',\n",
       " 'b r a v e l yEND',\n",
       " 'a c c t u al l yEND',\n",
       " 'a c u al l yEND',\n",
       " 'b a s i c l yEND',\n",
       " 'ha f fEND',\n",
       " 'in st in c t i v e l yEND',\n",
       " 'u n c on s c i ou s l yEND',\n",
       " 's in g l e - h an d e d l yEND',\n",
       " 's l y l yEND',\n",
       " 'a c t a u l l yEND',\n",
       " 'd r u n k en l yEND',\n",
       " 'a c u t al l yEND',\n",
       " 'f o o l i s h l yEND',\n",
       " 'b e ar l yEND',\n",
       " 'p u r p o s e f u l l yEND',\n",
       " 'j o k in g l yEND',\n",
       " 'r ou t in e l yEND',\n",
       " 'k n ow in g l yEND',\n",
       " 's u b c on s c i ou s l yEND',\n",
       " 'u n k n ow in g l yEND',\n",
       " 'a c t u l l yEND',\n",
       " 'm i r a c u l ou s l yEND',\n",
       " 'l it t er al l yEND',\n",
       " 'c o l l e c t i v e l yEND',\n",
       " 't r a d it i on al l yEND',\n",
       " 'in a d v er t en t l yEND',\n",
       " 'm i st a k en l yEND',\n",
       " 'v o l u n t ar i l yEND',\n",
       " 'b l in d l yEND',\n",
       " 'in d i re c t l yEND',\n",
       " 's p on t an e ou s l yEND',\n",
       " 'w i l l in g l yEND',\n",
       " 'h e re b yEND',\n",
       " 'g r a d u al l yEND',\n",
       " 'a c t u al yEND',\n",
       " 'j e sEND',\n",
       " 'd e l i b er a t e l yEND',\n",
       " 'c on t in u ou s l yEND',\n",
       " 's e l d om END',\n",
       " 'in t en t i on al l yEND',\n",
       " 'p u r p o s e l yEND',\n",
       " 'b ar l e yEND',\n",
       " 'in it i al l yEND',\n",
       " 'a c t i v e l yEND',\n",
       " 'on tEND',\n",
       " 'c a s u al l yEND',\n",
       " 'e s s en t i al l yEND',\n",
       " 'p r ou d l yEND',\n",
       " 't y p i c al l yEND',\n",
       " 'a c c i d en t l yEND',\n",
       " 'm a g i c al l yEND',\n",
       " 'al l e g e d l yEND',\n",
       " 's u p p o s e d l yEND',\n",
       " 'or i g in al l yEND',\n",
       " 's e c re t l yEND',\n",
       " 'g en er al l yEND',\n",
       " 'r a re l yEND',\n",
       " 'a u t om a t i c al l yEND',\n",
       " 'a c c i d en t al l yEND',\n",
       " 'r an d om l yEND',\n",
       " 'n or m al l yEND',\n",
       " 'c on st an t l yEND',\n",
       " 'ha r d l yEND',\n",
       " 'b a re l yEND',\n",
       " 'b a s i c al l yEND',\n",
       " 'l it er al l yEND',\n",
       " 'a c t u al l yEND',\n",
       " 'u s u al l yEND',\n",
       " 's u d d en t l yEND',\n",
       " '- al w a y sEND',\n",
       " 'al o sEND',\n",
       " 'c o in c i d en t l yEND',\n",
       " 'p r a y er f u l l yEND',\n",
       " 'd e m on b r u en END',\n",
       " 'd e s p ar a t e l yEND',\n",
       " 'g i e sEND',\n",
       " 'd e s p er a t l yEND',\n",
       " 'a s l oEND',\n",
       " 'al t er n a t e l yEND',\n",
       " 'b e l a t e d l yEND',\n",
       " 'j x END',\n",
       " 's u b s e q u en t l yEND',\n",
       " 's in c er l yEND',\n",
       " 'h en c e f or th END',\n",
       " 'u l t i m a t e l yEND',\n",
       " 'd e s p er a t e l yEND',\n",
       " 's in c e re l yEND',\n",
       " 's u d d en l yEND',\n",
       " 'al s oEND',\n",
       " 'o b i ou s l yEND',\n",
       " 'd e a d a z z END',\n",
       " 'e v i d en t al l yEND',\n",
       " 't o t al l y y yEND',\n",
       " 's r i ou s l yEND',\n",
       " 'o b v i ou l s yEND',\n",
       " 'o b v s l yEND',\n",
       " 's er o i u s l yEND',\n",
       " 'n e v er r r r r r rEND',\n",
       " 's er i s ou l yEND',\n",
       " 'h on e st l y yEND',\n",
       " 's r l s yEND',\n",
       " 's er i ou s l y y y yEND',\n",
       " 'l it er al l l yEND',\n",
       " 'o b v i o s l yEND',\n",
       " 'o v i ou s l yEND',\n",
       " 'g o t t c h aEND',\n",
       " 'o b v z END',\n",
       " 'd / aEND',\n",
       " 'w i s h iEND',\n",
       " 'g o t c ha aEND',\n",
       " 'i i i i i i i i iEND',\n",
       " '# j u st c a u s e w e c o o lEND',\n",
       " \"s or r y ' sEND\",\n",
       " 'l i k e e e e e eEND',\n",
       " 's h e e e eEND',\n",
       " 'g o t c h y aEND',\n",
       " 's r l yEND',\n",
       " 'f er re a lEND',\n",
       " 's er i ou s l y y yEND',\n",
       " 's i r i u s l yEND',\n",
       " 'g e z END',\n",
       " 'n e v er r r r r rEND',\n",
       " 'l o k e yEND',\n",
       " 's u r l e yEND',\n",
       " 's er i ou s l l yEND',\n",
       " 'h on e s l t yEND',\n",
       " 's er i u o s l yEND',\n",
       " 'd e a d a s s sEND',\n",
       " 'l ow k e y yEND',\n",
       " '- re al l yEND',\n",
       " 's er i u s l yEND',\n",
       " 's er i ou l yEND',\n",
       " 'b e t c h u END',\n",
       " 's er ou s l yEND',\n",
       " 's er z l yEND',\n",
       " 'h i g h k e yEND',\n",
       " 'n e v er r r r rEND',\n",
       " 's er i o s l yEND',\n",
       " 'l i k e e e e eEND',\n",
       " 's er i ou l s yEND',\n",
       " 's er i o s u l yEND',\n",
       " 'p er s on al yEND',\n",
       " 'u n d er st an d a b l yEND',\n",
       " 'u n n oEND',\n",
       " 's er i ou s l y yEND',\n",
       " 'g e d d i tEND',\n",
       " 'l i k e e e eEND',\n",
       " 'th e o re t i c al l yEND',\n",
       " 'd . aEND',\n",
       " 're al i st i c al l yEND',\n",
       " 'o b v iEND',\n",
       " 't r u th f u l l yEND',\n",
       " 'o b v sEND',\n",
       " 'b e t c h aEND',\n",
       " '# l ow k e yEND',\n",
       " 'o b v END',\n",
       " ...]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preform_bpe(brown_df, 50, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Jakie angielskie słowo jako pierwsze dostało swój własny token?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9820809857459406da2f488dccfd46ea",
     "grade": true,
     "grade_id": "cell-acd48c77e2c1bcec",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Jako czwarte zostało połączone angielskie słowo: `in`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Jakie są zalety korzystania z tokenizacji BPE w kontekście tworzenia reprezentacji do problemu klasyfikacji tekstu (problem OOV, odnieś się do k-gramów i n-gramów)? Jakie są zalety BPE w przypadku przetwarzania różny rodzajów języków (np. fleksyjne, aglutynacyjne)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e33a81bb178438ba45765ec7a9b31ab7",
     "grade": true,
     "grade_id": "cell-006ef6fd3e397206",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Niewątpliwą zaletą tokenizacji BPE jest rozwiązanie problemu OOV. Algorytm tworząc słownik połączonych ze sobą części wyrazów, w prosty sposób jest w stanie podzielić nieznany mu wyraz na mniejsze, znane już mu zbiory liter. Inną zaletą jest również łatwa możliwość dostosowania algorytmu do k-gramów i n-gramów, co może wpłynąć na wynik poprzez to, że uwzględniany będzie szerszy kontekst. BPE jest również skuteczny w językach fleksyjnych i aglutynacyjnych. Sposób działania algorytmu powoduje oddzielanie się od siebie morfemów, a więc `główna_część_wyrazu - affiks` również będą oddzielone ze względu na to jak często będą występowały w korpusie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wróć do implementacji i zakomentuj wypisywanie (funkcje print) informacji z funkcji `preform_bpe`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 2 - klasyfikacja (15 pkt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniższy kod powinien wczytać i ztokenizować zbiór danych dot. analizy wydźwięku. Jeśli nie masz biblioteki `nltk` musisz ją zainstalować."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import DataSet\n",
    "training_set = DataSet(['tweets.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej znajdziesz przykład odczytu jednego tweeta z obiektu DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in training_set.tweets:\n",
    "    print(i.text)\n",
    "    print(i.tokens)\n",
    "    print(i.clazz)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Systemy IL często pracują z bardzo dużą liczbą cech, które są rzadkie np. cechy Bag-Of-Words, cechy n-gramowe itd. Powoduje to że klasyczna macierz zawierająca przykłady uczące ($n$) i cechy ($d$) rośnie do bardzo dużych rozmiarów ($nd$) nawet dla małych zbiorów uczących (w sensie liczby przykładów). Ponadto samo przechowywanie w pamięci słownika mapującego konkretne słowa/n-gramy na indeksy kolumn macierzy może być bardzo kosztowne pamięciowo przy dużych rozmiarach słownika.\n",
    "\n",
    "Istnieje jednak technika, która pozwala nam na ominięcie tej przeszkody: haszowanie cech. Opis tej techniki znajdziesz na stronie:  https://en.wikipedia.org/wiki/Feature_hashing Jest ona też implementowana w obiekcie `sklearn.feature_extraction.FeatureHasher`. Zapoznaj się z opisem techniki i wykonaj poniższe polecenia.\n",
    "\n",
    "- Wykorzystując haszowanie cech wytrenuj wybrany klasyfikator (najlepiej taki, który się szybko liczy) na zbiorze uczącym dla cech Bag-of-words (możesz też spróbować cechy n-gramowe). Możesz wykorzystać gotową tokenizację we właściwości `.tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9f7ee33b52c8d037cb08c37c5b60221",
     "grade": true,
     "grade_id": "cell-f6cfe39258fbec51",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# WPISZ TWÓJ KOD TUTAJ\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stwórz wykres zależności wybranej miary klasyfikacji od wymiarów macierzy danych (chodzi o liczbę cech do których haszujemy cechy oryginalne). Wystarczy przetestować kilka (>=4) wybranych wartości na skali logarytmicznej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44e0a766a71ad8e4db609bffd5f0226b",
     "grade": true,
     "grade_id": "cell-8076c16242981ae9",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# WPISZ TWÓJ KOD TUTAJ\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Obserwując stworzony wykres - skomentuj. Jak dużo jakości klasyfikacji się traci (albo zyskuje?) korzystając z mniejszej liczby haszowanych cech? Często klasyfikatory bardzo dobrze działają nawet przy liczbie haszowanych cech dla których na pewno istnieją konflikty cech oryginalnych - jak myślisz dlaczego? (Pomyśl o interpretacji takich skonfliktowanych cech)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9988a3ddeab833b0356d93076f71ea02",
     "grade": true,
     "grade_id": "cell-2caea1821af5d8aa",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "WPISZ TWOJĄ ODPOWIEDŹ TUTAJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20139da166319b49eea5cc7e984fc08e",
     "grade": false,
     "grade_id": "cell-0d86672dbabbf54d",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    " - W poprzednim zadaniu wczytałeś wynik grupowania Browna do pamięci. Wytrenuj klasyfikator na reprezentacji ,,Bag-of-clusters'' tj. w kolumnach zamiast słów/n-gramów będziesz miał grupy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e9a1937bb7764ce939f7c24a4602e44",
     "grade": true,
     "grade_id": "cell-55264f6fe514d007",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# WPISZ TWÓJ KOD TUTAJ\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e47a053ebc12ac2fd97d9c11187da9b",
     "grade": false,
     "grade_id": "cell-493071698fc0205e",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "- Podsumuj eksperymenty: poznałeś dwie możliwości ograniczenia liczby cech - zastąpienie słów ich grupami i haszowanie cech. Jakie są wady i zalety obydwu podejść?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c65fee47d5ae3d664270ccfd0f3f605a",
     "grade": true,
     "grade_id": "cell-4508400659f7243e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "WPISZ TWOJĄ ODPOWIEDŹ TUTAJ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
