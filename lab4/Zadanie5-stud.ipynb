{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten notebook jest oceniany półautomatycznie. Nie twórz ani nie usuwaj komórek - struktura notebooka musi zostać zachowana. Odpowiedź wypełnij tam gdzie jest na to wskazane miejsce - odpowiedzi w innych miejscach nie będą sprawdzane (nie są widoczne dla sprawdzającego w systemie).\n",
    "\n",
    "W szczególności zwróć uwagę, że usupełniłeś wszystkie miejsca `YOUR CODE HERE`, `WPISZ TWÓJ KOD TUTAJ`, \"YOUR ANSWER HERE\" lub \"WPISZ TWOJĄ ODPOWIEDŹ TUTAJ\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moduł 4: Statystyczne tłumaczenie maszynowe\n",
    "\n",
    "## Zadanie 1\n",
    "Zadanie polega na zaimplementowaniu algorytmu Expectation-Maximization w modelu IBM Model 1 do przypasowywania słów. Będzie to fragment modelu, który tłumaczyć będzie z hiszpańskiego na angielski. \n",
    "\n",
    "UWAGA: Specjalny token \"NULL\" pomijamy w implementacji.\n",
    "\n",
    "Dany jest mini-korpus równoległy angielsko-hiszpański\n",
    "- \"green house\" \"casa verde\"\n",
    "- \"the house\" \"la casa\"\n",
    "- \"the green house\" \"la casa verde\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download de_core_news_sm\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import itertools\n",
    "english = [[\"green\",\"house\"], [\"the\",\"house\"], [\"the\", \"green\", \"house\"]]\n",
    "spanish = [[\"casa\", \"verde\"], [\"la\", \"casa\"], [\"la\", \"casa\", \"verde\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W dalszych funkcjach przydatne może być wyznaczenie słownika czyli zbioru słów z korpusu dla danego języka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b7bc060e49fc0e5843910db28871308",
     "grade": false,
     "grade_id": "cell-91ce5ad3201d3840",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def get_vocabulary(corpus):\n",
    "    \"\"\"\n",
    "    Funkcja zwracająca listę unikalnych słów z korpusu podanego w formacie zmiennej english i spanish\n",
    "    \"\"\"\n",
    "    return list(chain.from_iterable(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03f2fde3e2385f2037bcb146ba7914e5",
     "grade": true,
     "grade_id": "cell-69a37d074c36bafa",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_set_equal\n",
    "assert_set_equal(set(get_vocabulary(english)), set([\"the\", \"green\", \"house\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zainicjalizuj rozkład prawdopodobieństwa tłumaczenia słów rozkładem jednorodnym. Ponieważ zależy nam na prostocie implementacji (a nie efektywności) możemy to prawdopodobieństwo zaimplementować jako zwykły słownik, który będzie przyjmował na wejściu krotkę dwóch słów. Słownik nazwij `translation_prob` z kluczami (słowo_es, słowo_en)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5caab7c746dedcca6bd5f2416560d81f",
     "grade": false,
     "grade_id": "cell-d90e10211e9d2c82",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def initalize_translation_prob(corpus1, corpus2):\n",
    "    return {(w_1, w_2): 1/len(corpus2) for w_2 in corpus1 for w_1 in corpus2}\n",
    "\n",
    "translation_prob = initalize_translation_prob(set(get_vocabulary(english)), set(get_vocabulary(spanish)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wypisz zaincjalizowany słownik, żeby upewnić się że wynik jest prawidłowy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('la', 'house'): 0.3333333333333333,\n",
       " ('verde', 'house'): 0.3333333333333333,\n",
       " ('casa', 'house'): 0.3333333333333333,\n",
       " ('la', 'green'): 0.3333333333333333,\n",
       " ('verde', 'green'): 0.3333333333333333,\n",
       " ('casa', 'green'): 0.3333333333333333,\n",
       " ('la', 'the'): 0.3333333333333333,\n",
       " ('verde', 'the'): 0.3333333333333333,\n",
       " ('casa', 'the'): 0.3333333333333333}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimplementuj pierwszy krok algorytmu EM. Wyznacz wartości oczekiwane zmiennych przypisania słowa we wszystkich zdaniach w korpusie (oznaczane na wykładzie jako `a`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5dc89f1ea31cb9393e70b258ff9a1b81",
     "grade": false,
     "grade_id": "cell-20e467cd48dc07f4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_expectation(corpora1, corpora2, translation_prob):\n",
    "    \"\"\"\n",
    "    Procedura wykonująca krok \"E\" algorytmu EM\n",
    "    Wynikiem powinny być wartości oczekiwane dla zmiennej przypisań słów w zdaniach \n",
    "    (reprezentacja dowolna, nieweryfikowana przez sprawdzarkę)\n",
    "    \"\"\"\n",
    "    expectation = []\n",
    "\n",
    "    for c2, c1, e_i in zip(corpora2, corpora1, range(len(corpora1))):\n",
    "        expectation.append({})\n",
    "        for w2 in c2:\n",
    "            total = sum([translation_prob[(w2, w1)] for w1 in c1])\n",
    "            \n",
    "            for w1 in c1:\n",
    "                tuple_temp = (w2, w1)\n",
    "                if tuple_temp not in expectation[e_i]:\n",
    "                    expectation[e_i][tuple_temp] = translation_prob[(w2, w1)] / total\n",
    "    \n",
    "    return expectation\n",
    "\n",
    "assignment_expected_values = calculate_expectation(english, spanish, translation_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wypisz wartości oczekiwane zmiennych przypisań, aby zobaczyć jak wyglądają. Powinny one również prezentować całkowity brak wiedzy o przypisaniach (rozkłady jednorodne)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{('casa', 'green'): 0.5,\n",
       "  ('casa', 'house'): 0.5,\n",
       "  ('verde', 'green'): 0.5,\n",
       "  ('verde', 'house'): 0.5},\n",
       " {('la', 'the'): 0.5,\n",
       "  ('la', 'house'): 0.5,\n",
       "  ('casa', 'the'): 0.5,\n",
       "  ('casa', 'house'): 0.5},\n",
       " {('la', 'the'): 0.3333333333333333,\n",
       "  ('la', 'green'): 0.3333333333333333,\n",
       "  ('la', 'house'): 0.3333333333333333,\n",
       "  ('casa', 'the'): 0.3333333333333333,\n",
       "  ('casa', 'green'): 0.3333333333333333,\n",
       "  ('casa', 'house'): 0.3333333333333333,\n",
       "  ('verde', 'the'): 0.3333333333333333,\n",
       "  ('verde', 'green'): 0.3333333333333333,\n",
       "  ('verde', 'house'): 0.3333333333333333}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assignment_expected_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimplementuj drugi krok algorytmu EM. Wyznacz nowe `translation_prob` na podstawie oczekiwanych wartości zmiennych przypisań."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9cd9a503b1510e8979c6881b97dd4fb9",
     "grade": false,
     "grade_id": "cell-5806efe4531f9f01",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def calculate_maximization(corpora1, corpora2, assignment_expected_values):\n",
    "    total_sum = {}\n",
    "    trans_dict = {}\n",
    "    for s in assignment_expected_values:\n",
    "        for tuple_temp in s:\n",
    "            w2, w1 = tuple_temp\n",
    "            \n",
    "            if w1 not in total_sum:\n",
    "                total_sum[w1] = 0\n",
    "            if tuple_temp not in trans_dict:\n",
    "                trans_dict[tuple_temp] = 0\n",
    "\n",
    "            total_sum[w1] += s[(w2, w1)]\n",
    "            trans_dict[tuple_temp] += s[(w2, w1)]\n",
    "\n",
    "    for t in trans_dict:\n",
    "        trans_dict[t] /= total_sum[t[1]]\n",
    "\n",
    "    return trans_dict\n",
    "\n",
    "\n",
    "translation_prob = calculate_maximization(english, spanish, assignment_expected_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d92bddac888f1ab9d18c6ac43260456",
     "grade": true,
     "grade_id": "cell-4d5309aeab35f6c7",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_almost_equal\n",
    "assert_almost_equal(translation_prob[('casa', 'house')], 4/9.)\n",
    "assert_almost_equal(translation_prob[('la', 'house')], 5/18.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wywołaj w pętli 10 kroków algorytmu EM i zaobserwuj jak zmieniają się prawdopodobieństwa dla tłumacznienia \"house\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('casa', 'house'), 0.4884829229547261), (('verde', 'house'), 0.255758538522637), (('la', 'house'), 0.255758538522637)]\n",
      "---\n",
      "[(('casa', 'house'), 0.5457026346909629), (('verde', 'house'), 0.2271486826545185), (('la', 'house'), 0.22714868265451843)]\n",
      "---\n",
      "[(('casa', 'house'), 0.6065598881415178), (('verde', 'house'), 0.19672005592924113), (('la', 'house'), 0.19672005592924105)]\n",
      "---\n",
      "[(('casa', 'house'), 0.6657430854592091), (('verde', 'house'), 0.1671284572703954), (('la', 'house'), 0.16712845727039538)]\n",
      "---\n",
      "[(('casa', 'house'), 0.7204578730894079), (('verde', 'house'), 0.139771063455296), (('la', 'house'), 0.139771063455296)]\n",
      "---\n",
      "[(('casa', 'house'), 0.7693767685977914), (('verde', 'house'), 0.1153116157011043), (('la', 'house'), 0.1153116157011043)]\n",
      "---\n",
      "[(('casa', 'house'), 0.812047157282144), (('verde', 'house'), 0.0939764213589279), (('la', 'house'), 0.0939764213589279)]\n",
      "---\n",
      "[(('casa', 'house'), 0.8485414255151226), (('verde', 'house'), 0.07572928724243876), (('la', 'house'), 0.07572928724243876)]\n",
      "---\n",
      "[(('casa', 'house'), 0.8792386640488387), (('verde', 'house'), 0.06038066797558059), (('la', 'house'), 0.06038066797558059)]\n",
      "---\n",
      "[(('casa', 'house'), 0.9046837882788514), (('verde', 'house'), 0.0476581058605743), (('la', 'house'), 0.0476581058605743)]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    assignment_expected_values = calculate_expectation(english, spanish, translation_prob)\n",
    "    translation_prob = calculate_maximization(english, spanish, assignment_expected_values)\n",
    "    print([(i,j) for i,j in translation_prob.items() if i[1] == \"house\"])\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spróbujmy wykorzystać algorytm EM do stworzenia prawdopodobieństw $t(\\text{angielski}|\\text{polski})$ na poniższym korpusie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "english2 = [[\"the\",\"dog\"], [\"the\",\"house\"], [\"the\", \"green\", \"house\"]]\n",
    "polish = [[\"pies\"], [\"dom\"], [\"zielony\", \"dom\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c323a29363f0022cbf22907ecfcfba25",
     "grade": false,
     "grade_id": "cell-cff38f444b9c8d1a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('the', 'pies'): 0.5, ('dog', 'pies'): 0.5, ('the', 'dom'): 0.4998976014607412, ('house', 'dom'): 0.4998976014607412, ('the', 'zielony'): 0.1681262111738051, ('green', 'zielony'): 0.6637475776523899, ('green', 'dom'): 0.00020479707851748478, ('house', 'zielony'): 0.1681262111738051}\n"
     ]
    }
   ],
   "source": [
    "translation_prob = initalize_translation_prob(set(get_vocabulary(polish)), set(get_vocabulary(english2)))\n",
    "\n",
    "for i in range(10):\n",
    "    assignment_expected_values = calculate_expectation(polish, english2, translation_prob)\n",
    "    translation_prob = calculate_maximization(polish, english2, assignment_expected_values)\n",
    "    \n",
    "print(translation_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdź jak wyglądają prawdopodobieństwa tłumaczeń po 10 iteracjach. \n",
    "\n",
    "UWAGA: wynik algorytmu należy zapisać do zmiennej `translation_prob` -- to ona będzie testowana podczas oceniania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('the', 'pies'): 0.5,\n",
       " ('dog', 'pies'): 0.5,\n",
       " ('the', 'dom'): 0.4998976014607412,\n",
       " ('house', 'dom'): 0.4998976014607412,\n",
       " ('the', 'zielony'): 0.1681262111738051,\n",
       " ('green', 'zielony'): 0.6637475776523899,\n",
       " ('green', 'dom'): 0.00020479707851748478,\n",
       " ('house', 'zielony'): 0.1681262111738051}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bec67383c743edb9dca135c7db9c915d",
     "grade": true,
     "grade_id": "cell-ebbb28e3bee0112e",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_almost_equal\n",
    "# tu są ukryte testy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenia**\n",
    "- Sprawdź czy gdybyś dodał słówko `NULL` to algorytm nauczyłby się wiązać słówko `NULL` z `the`, które nie występuje w języku polskim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "15eaa6a5ef5c9ee228c291a25f163209",
     "grade": true,
     "grade_id": "cell-874a2c94ce1bd996",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('the', 'NULL'): 0.9370242904733792, ('the', 'pies'): 0.09940427160822979, ('dog', 'NULL'): 0.0002794544868163832, ('dog', 'pies'): 0.9005957283917702, ('the', 'dom'): 0.15423765076184237, ('house', 'NULL'): 0.06249885711871718, ('house', 'dom'): 0.8430994831807286, ('the', 'zielony'): 0.023160485778361987, ('green', 'NULL'): 0.00019739792108716355, ('green', 'zielony'): 0.8932057647751056, ('green', 'dom'): 0.002662866057429016, ('house', 'zielony'): 0.08363374944653247}\n"
     ]
    }
   ],
   "source": [
    "english2 = [[\"the\",\"dog\"], [\"the\",\"house\"], [\"the\", \"green\", \"house\"]]\n",
    "polish2 = [[\"NULL\", \"pies\"], [\"NULL\", \"dom\"], [\"NULL\", \"zielony\", \"dom\"]]\n",
    "\n",
    "translation_prob = initalize_translation_prob(set(get_vocabulary(polish2)), set(get_vocabulary(english2)))\n",
    "\n",
    "for i in range(10):\n",
    "    assignment_expected_values = calculate_expectation(polish2, english2, translation_prob)\n",
    "    translation_prob = calculate_maximization(polish2, english2, assignment_expected_values)\n",
    "    \n",
    "print(translation_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('the', 'NULL'): 0.9370242904733792,\n",
       " ('the', 'pies'): 0.09940427160822979,\n",
       " ('dog', 'NULL'): 0.0002794544868163832,\n",
       " ('dog', 'pies'): 0.9005957283917702,\n",
       " ('the', 'dom'): 0.15423765076184237,\n",
       " ('house', 'NULL'): 0.06249885711871718,\n",
       " ('house', 'dom'): 0.8430994831807286,\n",
       " ('the', 'zielony'): 0.023160485778361987,\n",
       " ('green', 'NULL'): 0.00019739792108716355,\n",
       " ('green', 'zielony'): 0.8932057647751056,\n",
       " ('green', 'dom'): 0.002662866057429016,\n",
       " ('house', 'zielony'): 0.08363374944653247}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Jeśli wywołałbyś EM dla pierwszego korpusu równoległego (zmienne `english` i `spanish`) i dołączył tokeny `NULL` to EM tłumaczy NULL jako \"casa\" i \"house\" jako \"casa\" z takimi samymi prawdopodobieństwami. Dlaczego?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0852bf8a51952bbe281c6f8053d2f8b0",
     "grade": true,
     "grade_id": "cell-0982ba49d3a2f657",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "WPISZ TWOJĄ ODPOWIEDŹ TUTAJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 2\n",
    "W tym zadaniu poznasz kolejne elementy biblioteki PyTorch. Konkretnie chodzi o operowanie na wymiarach tensorów, przełącznik `model.training` oraz mnożenie macierzowe paczek danych. Są to trzy oderwane od siebie tematy, ale wszystkie z nich będą potrzebne do implementacji kolejnego zadania.\n",
    "\n",
    "**Przełącznik model.training**\n",
    "Niektóre elementy sieci neuronowych takie jak dropout czy batch normalization inaczej działają w trakcie treningu modelu a inaczej w czasie predykcji. Korzystanie z tych warstw byłoby skomplikowane, gdyż wymagałoby innej obsługi w procedurze `forward`. Z tego powodu każdy z obiektów `nn.Module` posiada właściwość `training`, domyślnie ustawioną na `True`, z której korzystają warstwy takie jak dropout by dostosować swoje działanie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(1,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "model = Model()\n",
    "print( model.training )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przełącznik ten możemy zmieniać przy pomocy funkcji `eval()`, przestawiając model w tryb ewaluacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print( model.training )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A przed treningiem modelu należy wywołać `train()`, przestawiając model w tryb nauki. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "print( model.training )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W dotychczasowych implementacjach modeli nie wykorzystywaliśmy tego przełącznika, gdyż nie korzystaliśmy z warstw których działanie zależało od tego czy dokonujemy predykcji czy ewaluacji. Dobrą praktyką jest jednak zawsze wywoływanie `model.train()` przed treningiem modelu (i odpowiednio `model.eval()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Widoki tensorów**\n",
    "\n",
    "Przy implementacji różnych warstw sieci neuronowych, często konieczne jest inne ułożenie danych w wymiarach tensora tak aby zgadzały się one z formatem wejścia do warstwy kolejnej. Dotychczas wystarczało korzystanie z funkcji `.view()`, która ustawia wymiarowość tensora na tę podaną w jej argumencie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "dane = torch.arange(0,12)\n",
    "print(dane)\n",
    "dane2 = dane.view(3,4)\n",
    "print(dane2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Większość funkcji w Pytorch (które nie są operacjami in-place, końcówka nazwy `_`) zwraca zupełnie nowy tensor. Czy tak jest też tym razem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3, 44,  5,  6,  7,  8,  9, 10, 11])\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [44,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "dane2[1,0] = 44\n",
    "print(dane)\n",
    "print(dane2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zmiana danych w tensorze `dane2` spowodowała automatyczną zmianę danych w tensorze oryginalnym, gdyż funkcja `view()` zwraca jedynie widok do tego samego tensora. Widoki pozwalają na operowanie na tensorach w wygodnym formacie bez konieczności kopiowania danych. Oprócz `.view()` istnieją także inne funkcje, które tworzą widoki tensorów. Poznajmy kilka najpopularniejszych.\n",
    "\n",
    "Funkcja `permute` przyjmuje na wejście permutację kolejnych liczb naturalnych od 0 do wymiarowości tensora i układa wymiary w ten właśnie sposób. Dla przykładu `dane2.permute(0,1)` zwróci identyczny widok, gdyż jako pierwszy wymiar ustawi wymiar 0, a jako drugi wymiar 1. Jednak już `dane2.permute(1,0)` dokona transpozycji macierzy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [44,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[ 0, 44,  8],\n",
      "        [ 1,  5,  9],\n",
      "        [ 2,  6, 10],\n",
      "        [ 3,  7, 11]])\n"
     ]
    }
   ],
   "source": [
    "print ( dane2.permute(0,1) )\n",
    "print ( dane2.permute(1,0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogicznie działa to z większą liczbą wymiarów, w szczególności z trzema. Jeśli więc mamy ułożone dane w postaci `[długość sekwencji, wielkość paczki danych, liczba cech]` i chcemy je ułożyć tak by indeksowanie po poszczególych elementach paczki było na pierwszym wymiarze wystarczy stworzyć odpowiedni widok poprzez `permute`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1],\n",
      "         [ 2,  3]],\n",
      "\n",
      "        [[44,  5],\n",
      "         [ 6,  7]],\n",
      "\n",
      "        [[ 8,  9],\n",
      "         [10, 11]]]) torch.Size([3, 2, 2])\n",
      "tensor([[[ 0,  1],\n",
      "         [44,  5],\n",
      "         [ 8,  9]],\n",
      "\n",
      "        [[ 2,  3],\n",
      "         [ 6,  7],\n",
      "         [10, 11]]]) torch.Size([2, 3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dane3 = dane.view(3,2,2)\n",
    "print(dane3, dane3.shape)\n",
    "dane3p = dane3.permute(1,0,2)\n",
    "print(dane3p, dane3p.shape)\n",
    "# Elementy są identyczne po odpowiednim spermutowaniu indeksów\n",
    "dane3[2,1,0] == dane3p[1,2,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kolejną przydatną funkcją tworzącą widoki jest `unsqueeze` (i `squeeze`), która dodaje dodatkowy wymiar (o rozmiarze 1) w odpowiednim miejscu. Przykładowo wektor dane, chcielibyśmy przetworzyć jakąś warstwą, ale tak by potraktowała to jako 1-elementa paczka elementów o wymiarowości 12 (macierz 1x12). W tym celu możemy dodać sztuczny wymiar na pierwszej pozycji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3, 44,  5,  6,  7,  8,  9, 10, 11])\n",
      "tensor([[ 0,  1,  2,  3, 44,  5,  6,  7,  8,  9, 10, 11]])\n",
      "torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "daneu = dane.unsqueeze(0)\n",
    "print(dane)\n",
    "print(daneu)\n",
    "print(daneu.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogicznie wykorzystaj funkcję `unsqueeze` aby uzyskać macierz 12x1 z wektora `dane`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a15f68a9c03e74fa4a025e7b13d05cef",
     "grade": true,
     "grade_id": "cell-32537683a38f635f",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3, 44,  5,  6,  7,  8,  9, 10, 11])\n",
      "tensor([[ 0],\n",
      "        [ 1],\n",
      "        [ 2],\n",
      "        [ 3],\n",
      "        [44],\n",
      "        [ 5],\n",
      "        [ 6],\n",
      "        [ 7],\n",
      "        [ 8],\n",
      "        [ 9],\n",
      "        [10],\n",
      "        [11]])\n",
      "torch.Size([12, 1])\n"
     ]
    }
   ],
   "source": [
    "daneu2 = dane.unsqueeze(1)\n",
    "print(dane)\n",
    "print(daneu2)\n",
    "print(daneu2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z korzystaniem z widoków mogą się jednak wiązać problemy wydajnościowe modeli uczenia maszynowego. Wiele intensywnych obliczeniowo operacji matematycznych można efektywnie zaimplementować wykorzystując lokalność danych tj. zakładając że kolejne dane tensora/macierzy/wektora ułożone są obok siebie w pamięci. Wyobraź sobie np. warstwę liniową odczytującą cechy przykładu uczącego które przylegają do siebie w pamięci, a przykładu którego kolejne cechy, wskutek permutacji wymiarów, leżą w odległych miejscach pamięci. \n",
    "\n",
    "W PyTroch możemy sprawdzić czy tensor zawiera dane, które przylegają do siebie w pamięci wywołując funkcję `is_contiguous()` (konkretnie dane powinny być ułożone w pamięci po kolei od skrajnie prawego wymiaru tj. w przypadku macierzy wierszowo). Jest też możliwe wywołanie funkcji `contiguous()`, która wymusza przyległe ułożenie danych w tensorze poprzez skopiowanie danych i stworzenie nowego tensora. Pomimo kosztu pamięciowego i czasowego, przed wykonaniem szczególnie kosztownych operacji obliczeniowych, wykonanie tej operacji może się opłacać. Można też uważnie operować na tensorach, by unikać nie-przylegających tensorów ;) Zwróć uwagę, że nie każdy widok od razu powoduje brak przyległości danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(daneu.is_contiguous())\n",
    "print(dane3p.is_contiguous())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mnożenie macierzowe z paczkami danych**\n",
    "\n",
    "Na koniec poznajmy przydatną funkcję `bmm` czyli *batch matrix multiplication*. Rozważmy sytuację w której mamy kolekcję 3 macierzy (a1, a2, a3) i potrzebujemy uzyskać wyniki mnożenia tych macierzy przez analogiczną kolekcję macierzy (b1, b2, b3). Możemy wykonać te mnożenia kolejno w pętli tj. a1@b1, a2@b2 i a3@b3, jednak istnieje także inna możliwość: spakować te macierze do tensorów i wykonać operację `bbm`, która zwróci nam dokładnie taki sam wynik (ale znacznie szybciej niż w przypadku iterowania)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[[1,0],[0,1]],  [[2,0],[0,2]],  [[1,1],[1,1]] ])\n",
    "B = torch.tensor([[[1,2],[3,4]],  [[-1,-2],[-3,-4]], [[1,2],[3,4]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1,  2],\n",
      "         [ 3,  4]],\n",
      "\n",
      "        [[-2, -4],\n",
      "         [-6, -8]],\n",
      "\n",
      "        [[ 4,  6],\n",
      "         [ 4,  6]]])\n",
      "A torch.Size([3, 2, 2]), B torch.Size([3, 2, 2]), result torch.Size([3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "result = torch.bmm(A,B)\n",
    "print(result)\n",
    "print(f\"A {A.shape}, B {B.shape}, result {result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 3\n",
    "Celem tego zadania jest zaimplementowanie prostej architektury koder-dekoder dla problemu tłumaczenia maszynowego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchtext\n",
      "  Using cached torchtext-0.16.2-cp310-cp310-win_amd64.whl (1.9 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\marcin\\appdata\\roaming\\python\\python310\\site-packages (from torchtext) (1.24.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\marcin\\appdata\\roaming\\python\\python310\\site-packages (from torchtext) (4.65.0)\n",
      "Requirement already satisfied: requests in c:\\users\\marcin\\appdata\\roaming\\python\\python310\\site-packages (from torchtext) (2.31.0)\n",
      "Collecting torchdata==0.7.1\n",
      "  Using cached torchdata-0.7.1-cp310-cp310-win_amd64.whl (1.3 MB)\n",
      "Requirement already satisfied: torch==2.1.2 in c:\\users\\marcin\\appdata\\roaming\\python\\python310\\site-packages (from torchtext) (2.1.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\marcin\\appdata\\roaming\\python\\python310\\site-packages (from torch==2.1.2->torchtext) (3.1.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\marcin\\appdata\\roaming\\python\\python310\\site-packages (from torch==2.1.2->torchtext) (3.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\marcin\\appdata\\roaming\\python\\python310\\site-packages (from torch==2.1.2->torchtext) (2023.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\marcin\\appdata\\roaming\\python\\python310\\site-packages (from torch==2.1.2->torchtext) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\marcin\\appdata\\roaming\\python\\python310\\site-packages (from torch==2.1.2->torchtext) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\marcin\\appdata\\roaming\\python\\python310\\site-packages (from torch==2.1.2->torchtext) (1.11.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\marcin\\appdata\\roaming\\python\\python310\\site-packages (from torchdata==0.7.1->torchtext) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\marcin\\appdata\\roaming\\python\\python310\\site-packages (from requests->torchtext) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\marcin\\appdata\\roaming\\python\\python310\\site-packages (from requests->torchtext) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\marcin\\appdata\\roaming\\python\\python310\\site-packages (from requests->torchtext) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\marcin\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->torchtext) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\marcin\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch==2.1.2->torchtext) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\marcin\\appdata\\roaming\\python\\python310\\site-packages (from sympy->torch==2.1.2->torchtext) (1.3.0)\n",
      "Installing collected packages: torchdata, torchtext\n",
      "Successfully installed torchdata-0.7.1 torchtext-0.16.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 127] Nie można odnaleźć określonej procedury",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Multi30k\n\u001b[0;32m      7\u001b[0m data \u001b[38;5;241m=\u001b[39m Multi30k(split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, language_pair\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mde\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      8\u001b[0m data_val \u001b[38;5;241m=\u001b[39m Multi30k(split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m, language_pair\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mde\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchtext\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _get_torch_home\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m      8\u001b[0m _TEXT_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m _CACHE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchtext\\_extension.py:64\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m \u001b[43m_init_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchtext\\_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils\u001b[38;5;241m.\u001b[39mis_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext._torchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext C++ Extension is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtorchtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchtext\\_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[1;34m(lib)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\_ops.py:643\u001b[0m, in \u001b[0;36mload_library\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m    642\u001b[0m     \u001b[38;5;66;03m# It is not a valid op_name when __file__ is passed in\u001b[39;00m\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    644\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.ops\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;66;03m# ensure that query for dunder attributes that does not exist on\u001b[39;00m\n\u001b[0;32m    647\u001b[0m     \u001b[38;5;66;03m# opoverloadpacket but instead exists on the self._op object does not unnecessarily call\u001b[39;00m\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;66;03m# `_get_operation_overload` (which is an expensive operation).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    651\u001b[0m     \u001b[38;5;66;03m# opoverloadpacket.\u001b[39;00m\n\u001b[0;32m    652\u001b[0m     \u001b[38;5;66;03m# This is ok since we are guaranteed that an overload name for an aten op can't start with '__'\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\ctypes\\__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] Nie można odnaleźć określonej procedury"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "data = Multi30k(split='train', language_pair=('de', 'en'))\n",
    "data_val = Multi30k(split='valid', language_pair=('de', 'en'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konieczna jest instalacja tokenizatorów dla angielskiego i niemieckiego (a wcześniej biblioteki `spacy` jeśli nie masz jej zainstalowanej)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download de_core_news_sm\n",
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klasa `ParallelCorpus` jest typu `Dataset` co pozwala na jej użycie w obiektach klasy `DataLoader`. Do jej konstruktora należy podać zbiór danych, dwa kody języków (źródło, cel) oraz (opcjonalnie) dwa obiekty typu `Vocab` i liczbę `data_limit`. Jeżeli obiekty `Vocab` nie zostaną podane to obiekt `ParallelCorpus` utworzy je automatycznie na podstawie zbioru danych. Jeśli zaś nie podasz `data_limit` to zostanie wczytany cały zbiór danych, w przeciwnym razie wczytanie zostanie tylko `data_limit` par zdań. W dalszej implementacji będziesz potrzebował odczytać wielkość słownika co możesz zrobić poprzez wywołanie `len()` na obiektach `vocab_a` i `vocab_b`, które są właściwością klasy `ParallelCorpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import ParallelCorpus\n",
    "\n",
    "dataset = ParallelCorpus(data,'de', 'en', data_limit = 10000)\n",
    "#W zbiorze walidacyjnym używamy tych samych słowników\n",
    "dataset_val = ParallelCorpus(data_val, 'de', 'en', dataset.vocab_a, dataset.vocab_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponieważ w porównaniu do poprzednich zadań obliczenia mogą być bardziej czasochłonne to możesz skorzystać z możliwości karty graficznej zgodnej z CUDA (jeśli taką posiadasz). Aby obliczenia odbywały się na karcie graficznej wszystkie tensory uczestniczące w operacji (np. parametry modelu oraz dane) muszą zostać w niej zapisane. Możesz to zrobić wywołując na modelu lub tensorze funkcję `.to(device)` której argumentem jest urządzenie (cpu lub karta graficzna). Poniższa linijka pozwala na sprawdzenie czy karta graficzna zgodna z CUDA jest dostępna oraz przypisanie odpowiedniego urządzenia. \n",
    "\n",
    "*UWAGA* Jeśli nie jesteś w posiadaniu karty graficznej z CUDA to możesz mocno zmniejszyć `data_limit`, a z kolei jeśli ją masz to możesz go podwyższyć. Ilość danych wpływa na jakość uzyskanych wyników, jednak ocenie podlega poprawność implementacji, a nie jakość wyników. W przypadku dalszych problemów możesz także zmniejszyć pojawiające się dalej stałe `HIDDEN_DIM` i `WORD_EMBEDDING` czy wielkość paczki danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Koder w naszej implementacji to dwukierunkowa sieć rekurencyjna typu GRU, do której wejściem są kolejne tokeny przetworzone przez macierz zanurzeń."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_EMBEDDING = 128\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, WORD_EMBEDDING)\n",
    "        self.rnn = nn.GRU(WORD_EMBEDDING, hidden_size // 2 , bidirectional = True)\n",
    "        \n",
    "    def forward(self, src, src_len):\n",
    "        embedded = self.embedding(src)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len)\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)         \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
    "        hidden = torch.cat([hidden[0,:, :], hidden[1,:,:]], dim=1)\n",
    "        return outputs, hidden.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogicznie, dekoder jest jednokierunkową siecią rekurencyjną typu GRU o takiej samej wymiarowości stanu ukrytego co enkoder. Z tego powodu stan ukryty kodera może zostać przekazany dekoderowi bez żadnej transformacji.\n",
    "\n",
    "Dekoder jest jednak trochę bardziej skomplikowany, gdyż w każdej iteracji na wejście otrzymuje kolejne słowo, które zostało przewidziane (!) w poprzedniej iteracji. Nie jest więc możliwe skolekcjonowanie całej sekwencji wejściowej i jednorazowe wywołanie `rnn(wejście)` tylko trzeba to robić element po elemencie. Dodatkowo wejściem do dekodera jest też wektor kontekstu, który powinien zostać stworzony przez mechanizm uwagi (obecnie nie jest on zaimplementowany).\n",
    "\n",
    "Podsumowując: wejściem do dekodera jest słowo z poprzedniej iteracji (przetworzone przez macierz zanurzeń), wektor kontekstu oraz stan ukryty dekodera z ostatniej iteracji (oprócz pierwszej w której wejściem jest ostatni stan kodera).\n",
    "\n",
    "Wyjście dekodera to z kolei przetłumaczone słowo, które jest w tej implementacji przewidywane na podstawie warstwy liniowej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        # Słowo z poprzedniej iteracji jest reprezentowane zanurzeniem \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, WORD_EMBEDDING)\n",
    "        self.rnn = nn.GRU(hidden_size + WORD_EMBEDDING, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, input, hidden, encoder_outputs, src_len):\n",
    "        # Funckję wywołujemy iteracyjnie tzn. tylko dla jednego słowa \n",
    "        # dla każdej z paczki przetwarzanych sekwencji\n",
    "        \n",
    "        # input = [batch size] (indeks tylko jednego słowa per zdanie)\n",
    "        # hidden = [batch size, hidden_size]\n",
    "        # encoder_outputs = [src len, batch size, hidden_size]\n",
    "        # mask = [batch size, src len]\n",
    "      \n",
    "        embedded = self.embedding(input)\n",
    "        \n",
    "        #Wektor zer - w przyszłości wynik mechanizmu uwagi\n",
    "        # context = [batch size, hidden_size]\n",
    "        context = torch.zeros((input.shape[0], self.rnn.input_size - WORD_EMBEDDING)).to(embedded.device)\n",
    "        \n",
    "        #Wejście do sieci do wektor kontekstu połączony z przetwarzanym słowem\n",
    "        rnn_input = torch.cat((embedded, context), dim = 1)\n",
    "        \n",
    "        # Wejście do sieci rekurencyjnej zawiera wymiar - długość przetwarzanej sekwencji\n",
    "        # Konieczne jest więc \"sztuczne\" dodanie wymiaru na pierwszej (tj. zerowej) pozycji\n",
    "        rnn_input = rnn_input.unsqueeze(0)\n",
    "        #rnn_input = [1, batch size, word_embedding + hidden_size]\n",
    "        \n",
    "        _, hidden = self.rnn(rnn_input, hidden)\n",
    "        \n",
    "        prediction = self.fc(hidden.squeeze(0))\n",
    "        \n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pytania sprawdzające zrozumienie implementacji**\n",
    "- Dlaczego podczas zwracania stanu ukrytego kodera musieliśmy połączyć jego dwa pierwsze wymiary, aby zainicjalizować nimi dekoder?\n",
    "- W implementacji kodera osobno zwróciliśmy `outputs`, oraz `hidden`. To ostatenie jeszcze musieliśmy połączyć w dłuższe wektory aby uzyskać jeden wektor stanu ukrytego dla każdego elementu paczki danych (gdyż sieć była dwukierunkowa). Czy nie prościej byłoby wziąć `outputs[-1]`? Podpowiedź: dla każdego $i>0$ `outputs[-1][i]` zawiera same zera.\n",
    "- W implementacji dekodera do warstwy liniowej przekazaliśmy `hidden.squeeze(0)` tj. usunęliśmy pierwszy wymiar tensora (który miał długość 1). Czy jest to wymiar związany z długością przetwarzanej sekwencji przez sieć rekurencyjną, który sztucznie dodaliśmy kilka linijek wyżej? Podpowiedź: nie, sprawdź  dokumentację neuronu GRU w Pytorch\n",
    "- Czy macierz zanurzeń dekodera i enkodera mogłaby być współdzielona? Tj. będzie zawierała reprezentacje tych samych słów?\n",
    "\n",
    "Ostatecznie implementujemy model koder-dekoder, który łączy dwa powyższe obiekty. W szczególności ważna jest funkcja `forward`, która iteracyjnie wywołuje dekoder dla kolejno przetwarzanych słów. Bardziej konkretnie: dla poprzedniego słowa w języku docelowym. W tym miejscu mamy do wyboru dwie stragie: pierwsza to podawanie na wejście dekodera najbardziej prawdopodobne słowo które poprzednio przewidział lub zastosowanie *teacher forcing*. W tej implementacji model znajdujący się w trybie ewaluacji będzie przekazywał dekoderowi przewidziane przez niego słowa, a w fazie treningu będzie korzystał z *teacher forcing*.\n",
    "\n",
    "Zwróć uwagę, że model zwraca całą serię wyjść z dekodera (zmienna `outputs`) - jest to konieczne by policzyć funkcję straty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = next(enc.parameters()).device\n",
    "        \n",
    "    def forward(self, src, src_len, tgt):       \n",
    "        tgt_paded_len = tgt.shape[0]\n",
    "        batch_size = tgt.shape[1]\n",
    "        vocab_tgt_size = self.decoder.vocab_size\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(tgt_paded_len, batch_size, vocab_tgt_size).to(self.device)\n",
    "        \n",
    "        # Przetworzenie wejścia przez koder, kolejno uzyskane reprezentacje\n",
    "        # kazdego słowa będą potrzebne do implementacji mechanizmu uwagi\n",
    "        # Z kolei hidden posłuży do inicjalizacji stanu ukrytego dekodera\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        prev_word = tgt[0,:]\n",
    "                \n",
    "        for i in range(1, tgt_paded_len):          \n",
    "            output, hidden = self.decoder(prev_word, hidden, encoder_outputs, src_len)\n",
    "            outputs[i] = output\n",
    "               \n",
    "            # uczenie poprzez teaching forcing\n",
    "            prev_word = tgt[i] if self.training else output.argmax(1)\n",
    "            \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ostatecznie tworzymy obiekty typu `DataLoader` z odpowiednia funkcją tworzącą paczkę danych (nie musisz jej analizować)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for sample in batch:\n",
    "        src_batch.append(sample[\"text_a\"])\n",
    "        tgt_batch.append(sample[\"text_b\"])\n",
    "    src_batch = pad_sequence(src_batch, padding_value=0)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=0)\n",
    "    lena_batch = torch.tensor([len(sample[\"text_a\"]) for sample in batch], dtype=torch.int64)\n",
    "    lenb_batch = torch.tensor([len(sample[\"text_b\"]) for sample in batch], dtype=torch.int64)\n",
    "    idx = torch.argsort(lena_batch, descending=True)\n",
    "    return src_batch[:,idx].to(device), tgt_batch[:,idx].to(device), lena_batch[idx], lenb_batch[idx]\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=64, collate_fn=collate_fn)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=256, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A także pętlę uczącą model, zwróć uwagę na ustawienie przełącznika `.train()`, stosowanie techniki normalizacji gradientu, ignorowanie w funkcji celu etykiet `<pad>` (uzupełnienie sekwencji do wspólnej długości) oraz na pominięcie pierwszego tokenu `<start>` w sekwencji docelowej. Pomimo tego, że w zbiorze danych każde zdanie zaczyna się od tokenu `<start>` i kończy się tokenem `<stop>`, i w związku z tym token `<start>` trafia na wejście dekodera w pierwszej iteracji, to dekoder od razu przewiduje kolejne słowo tłumaczonego zdania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 256\n",
    "EPOCHS = 3\n",
    "\n",
    "enc = Encoder(len(dataset.vocab_a), HIDDEN_DIM).to(device)\n",
    "dec = Decoder(len(dataset.vocab_b), HIDDEN_DIM).to(device)\n",
    "enc_dec = EncoderDecoder(enc, dec).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(enc_dec.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "\n",
    "enc_dec.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        src, tgt, src_len, tgt_len = batch\n",
    "        \n",
    "        outputs = enc_dec(src, src_len, tgt)\n",
    "\n",
    "        # Token START nie jest przewidywany przez dekoder\n",
    "        tgt = tgt[1:].view(-1)\n",
    "        \n",
    "        # Tablekę outputs zaczęliśmy uzupełniać od indeksu 1\n",
    "        outputs = outputs[1:].view(-1, dec.vocab_size)\n",
    "\n",
    "        loss = criterion(outputs, tgt)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Ucinanie gradientu\n",
    "        torch.nn.utils.clip_grad_norm_(enc_dec.parameters(), 2.)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        epoch_loss += loss   \n",
    "    print(f'Epoch: {epoch+1:02} | Loss: {epoch_loss / len(dataloader):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykonanie powyższych 3 epok uczenia może zająć na szybkim CPU ok. 1,5 minuty.\n",
    "\n",
    "Sprawdź jak radzi sobie model z przetłumaczeniem przykładowego zdania ze zbioru uczącego (oczywiście, wynik może być  daleki od optymalnego z powodu krótkiego czasu treningu i pomniejszonego zbioru danych). Następnie sprawdź jak sobie model radzi z tłumaczeniem wybranego zdania ze zbioru walidacyjnego (obiekt `dataset_val`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import translate\n",
    "example_idx = 12\n",
    "\n",
    "src = dataset[example_idx][\"text_a\"]\n",
    "translate(src, enc_dec, dataset, device)\n",
    "\n",
    "tgt =  dataset[example_idx][\"text_b\"]\n",
    "print(f'Referencja = {dataset.vocab_b.lookup_tokens(tgt.numpy())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Zaimplemenuj funkcję, która policzy średnią wartość entropii krzyżowej na zbiorze walidacyjnym, która byłaby przydatna np. do wybrania odpowiedniej liczby epok uczących.\n",
    "- pamiętaj o przełączeniu modelu w tryb ewaluacji\n",
    "- pamiętaj o nieśledzeniu wartości gradientów `with torch.no_grad()`\n",
    "- wykorzystaj już wcześniej zainicjalizowany `dataloader_val`, który iteruje po paczkach zbioru walidacyjnego\n",
    "- na koniec dopisz wywołanie twojej funkcji do powyższej pętli uczącej algorytm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e942d80c24b7c174b65a100e73c800be",
     "grade": true,
     "grade_id": "cell-bf4c15791c51a28f",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_validation_set(model, dataloader, criterion):\n",
    "# WPISZ TWÓJ KOD TUTAJ\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozbuduj dekoder o mechanizm uwagi (tj. o obliczanie wektora `context`) przedstawiony na wykładzie, gdzie do policzenia wag uwagi jest wykorzystany zwykły iloczyn skalarny. Stan ukryty dekodera `hidden` wykorzystaj jako wektor `q`, a stany ukryte kodera dla każdego elementu zdania wejściowego są w tensorze `encoder_outputs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "add31f3b44b55ac1b0a74bbf8473405d",
     "grade": true,
     "grade_id": "cell-257a4a58e55953f8",
     "locked": false,
     "points": 14,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        # Słowo z poprzedniej iteracji jest reprezentowane zanurzeniem \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, WORD_EMBEDDING)\n",
    "        self.rnn = nn.GRU(hidden_size + WORD_EMBEDDING, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, input, hidden, encoder_outputs, src_len):\n",
    "        # Funckję wywołujemy iteracyjnie tzn. tylko dla jednego słowa \n",
    "        # dla każdej z paczki przetwarzanych sekwencji\n",
    "        \n",
    "        # input = [batch size] (indeks tylko jednego słowa per zdanie)\n",
    "        # hidden = [batch size, hidden_size]\n",
    "        # encoder_outputs = [src len, batch size, hidden_size]\n",
    "        # mask = [batch size, src len]\n",
    "      \n",
    "        embedded = self.embedding(input)\n",
    "        \n",
    "# WPISZ TWÓJ KOD TUTAJ\n",
    "raise NotImplementedError()\n",
    "        # context = [batch size, hidden_size]\n",
    "        # context = ....\n",
    "        \n",
    "        #Wejście do sieci do wektor kontekstu połączony z przetwarzanym słowem\n",
    "        rnn_input = torch.cat((embedded, context), dim = 1)\n",
    "        \n",
    "        # Wejście do sieci rekurencyjnej zawiera wymiar - długość przetwarzanej sekwencji\n",
    "        # Konieczne jest więc \"sztuczne\" dodanie wymiaru na pierwszej (tj. zerowej) pozycji\n",
    "        rnn_input = rnn_input.unsqueeze(0)\n",
    "        #rnn_input = [1, batch size, word_embedding + hidden_size]\n",
    "        \n",
    "        _, hidden = self.rnn(rnn_input, hidden)\n",
    "        \n",
    "        prediction = self.fc(hidden.squeeze(0))\n",
    "        \n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenia**\n",
    "- Porównaj wyniki uzyskane modelem z mechanizmem uwagi z wynikami bez niego. Czy udało się osiągnąć lepszą jakość tłumaczenia lub unika się pewnych rodzajów błędów?\n",
    "- Jak wpływa dodanie mechanizmu uwagi na dynamikę uczenia się? Trwa ono dużej/krócej? Funkcja celu spada szybciej czy wolniej?\n",
    "- Jakie czynniki są według Ciebie kluczowe jeśli chodzi o szybkość treningu systemów NMT a także wymagania pamięciowe dla nich?\n",
    "\n",
    "Odpowedzi na powyższe nie musisz wpisywać."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
