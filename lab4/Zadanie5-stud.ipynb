{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten notebook jest oceniany półautomatycznie. Nie twórz ani nie usuwaj komórek - struktura notebooka musi zostać zachowana. Odpowiedź wypełnij tam gdzie jest na to wskazane miejsce - odpowiedzi w innych miejscach nie będą sprawdzane (nie są widoczne dla sprawdzającego w systemie).\n",
    "\n",
    "W szczególności zwróć uwagę, że usupełniłeś wszystkie miejsca `YOUR CODE HERE`, `WPISZ TWÓJ KOD TUTAJ`, \"YOUR ANSWER HERE\" lub \"WPISZ TWOJĄ ODPOWIEDŹ TUTAJ\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moduł 4: Statystyczne tłumaczenie maszynowe\n",
    "\n",
    "## Zadanie 1\n",
    "Zadanie polega na zaimplementowaniu algorytmu Expectation-Maximization w modelu IBM Model 1 do przypasowywania słów. Będzie to fragment modelu, który tłumaczyć będzie z hiszpańskiego na angielski. \n",
    "\n",
    "UWAGA: Specjalny token \"NULL\" pomijamy w implementacji.\n",
    "\n",
    "Dany jest mini-korpus równoległy angielsko-hiszpański\n",
    "- \"green house\" \"casa verde\"\n",
    "- \"the house\" \"la casa\"\n",
    "- \"the green house\" \"la casa verde\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download de_core_news_sm\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import itertools\n",
    "english = [[\"green\",\"house\"], [\"the\",\"house\"], [\"the\", \"green\", \"house\"]]\n",
    "spanish = [[\"casa\", \"verde\"], [\"la\", \"casa\"], [\"la\", \"casa\", \"verde\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W dalszych funkcjach przydatne może być wyznaczenie słownika czyli zbioru słów z korpusu dla danego języka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b7bc060e49fc0e5843910db28871308",
     "grade": false,
     "grade_id": "cell-91ce5ad3201d3840",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def get_vocabulary(corpus):\n",
    "    \"\"\"\n",
    "    Funkcja zwracająca listę unikalnych słów z korpusu podanego w formacie zmiennej english i spanish\n",
    "    \"\"\"\n",
    "    return list(chain.from_iterable(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03f2fde3e2385f2037bcb146ba7914e5",
     "grade": true,
     "grade_id": "cell-69a37d074c36bafa",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_set_equal\n",
    "assert_set_equal(set(get_vocabulary(english)), set([\"the\", \"green\", \"house\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zainicjalizuj rozkład prawdopodobieństwa tłumaczenia słów rozkładem jednorodnym. Ponieważ zależy nam na prostocie implementacji (a nie efektywności) możemy to prawdopodobieństwo zaimplementować jako zwykły słownik, który będzie przyjmował na wejściu krotkę dwóch słów. Słownik nazwij `translation_prob` z kluczami (słowo_es, słowo_en)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5caab7c746dedcca6bd5f2416560d81f",
     "grade": false,
     "grade_id": "cell-d90e10211e9d2c82",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def initalize_translation_prob(corpus1, corpus2):\n",
    "    return {(w_1, w_2): 1/len(corpus2) for w_2 in corpus1 for w_1 in corpus2}\n",
    "\n",
    "translation_prob = initalize_translation_prob(set(get_vocabulary(english)), set(get_vocabulary(spanish)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wypisz zaincjalizowany słownik, żeby upewnić się że wynik jest prawidłowy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('la', 'house'): 0.3333333333333333,\n",
       " ('verde', 'house'): 0.3333333333333333,\n",
       " ('casa', 'house'): 0.3333333333333333,\n",
       " ('la', 'green'): 0.3333333333333333,\n",
       " ('verde', 'green'): 0.3333333333333333,\n",
       " ('casa', 'green'): 0.3333333333333333,\n",
       " ('la', 'the'): 0.3333333333333333,\n",
       " ('verde', 'the'): 0.3333333333333333,\n",
       " ('casa', 'the'): 0.3333333333333333}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimplementuj pierwszy krok algorytmu EM. Wyznacz wartości oczekiwane zmiennych przypisania słowa we wszystkich zdaniach w korpusie (oznaczane na wykładzie jako `a`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5dc89f1ea31cb9393e70b258ff9a1b81",
     "grade": false,
     "grade_id": "cell-20e467cd48dc07f4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_expectation(corpora1, corpora2, translation_prob):\n",
    "    \"\"\"\n",
    "    Procedura wykonująca krok \"E\" algorytmu EM\n",
    "    Wynikiem powinny być wartości oczekiwane dla zmiennej przypisań słów w zdaniach \n",
    "    (reprezentacja dowolna, nieweryfikowana przez sprawdzarkę)\n",
    "    \"\"\"\n",
    "    expectation = []\n",
    "\n",
    "    for c2, c1, e_i in zip(corpora2, corpora1, range(len(corpora1))):\n",
    "        expectation.append({})\n",
    "        for w2 in c2:\n",
    "            total = sum([translation_prob[(w2, w1)] for w1 in c1])\n",
    "            \n",
    "            for w1 in c1:\n",
    "                tuple_temp = (w2, w1)\n",
    "                if tuple_temp not in expectation[e_i]:\n",
    "                    expectation[e_i][tuple_temp] = translation_prob[(w2, w1)] / total\n",
    "    \n",
    "    return expectation\n",
    "\n",
    "assignment_expected_values = calculate_expectation(english, spanish, translation_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wypisz wartości oczekiwane zmiennych przypisań, aby zobaczyć jak wyglądają. Powinny one również prezentować całkowity brak wiedzy o przypisaniach (rozkłady jednorodne)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{('casa', 'green'): 0.5,\n",
       "  ('casa', 'house'): 0.5,\n",
       "  ('verde', 'green'): 0.5,\n",
       "  ('verde', 'house'): 0.5},\n",
       " {('la', 'the'): 0.5,\n",
       "  ('la', 'house'): 0.5,\n",
       "  ('casa', 'the'): 0.5,\n",
       "  ('casa', 'house'): 0.5},\n",
       " {('la', 'the'): 0.3333333333333333,\n",
       "  ('la', 'green'): 0.3333333333333333,\n",
       "  ('la', 'house'): 0.3333333333333333,\n",
       "  ('casa', 'the'): 0.3333333333333333,\n",
       "  ('casa', 'green'): 0.3333333333333333,\n",
       "  ('casa', 'house'): 0.3333333333333333,\n",
       "  ('verde', 'the'): 0.3333333333333333,\n",
       "  ('verde', 'green'): 0.3333333333333333,\n",
       "  ('verde', 'house'): 0.3333333333333333}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assignment_expected_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimplementuj drugi krok algorytmu EM. Wyznacz nowe `translation_prob` na podstawie oczekiwanych wartości zmiennych przypisań."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9cd9a503b1510e8979c6881b97dd4fb9",
     "grade": false,
     "grade_id": "cell-5806efe4531f9f01",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def calculate_maximization(corpora1, corpora2, assignment_expected_values):\n",
    "    total_sum = {}\n",
    "    trans_dict = {}\n",
    "    for s in assignment_expected_values:\n",
    "        for tuple_temp in s:\n",
    "            w2, w1 = tuple_temp\n",
    "            \n",
    "            if w1 not in total_sum:\n",
    "                total_sum[w1] = 0\n",
    "            if tuple_temp not in trans_dict:\n",
    "                trans_dict[tuple_temp] = 0\n",
    "\n",
    "            total_sum[w1] += s[(w2, w1)]\n",
    "            trans_dict[tuple_temp] += s[(w2, w1)]\n",
    "\n",
    "    for t in trans_dict:\n",
    "        trans_dict[t] /= total_sum[t[1]]\n",
    "\n",
    "    return trans_dict\n",
    "\n",
    "\n",
    "translation_prob = calculate_maximization(english, spanish, assignment_expected_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d92bddac888f1ab9d18c6ac43260456",
     "grade": true,
     "grade_id": "cell-4d5309aeab35f6c7",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_almost_equal\n",
    "assert_almost_equal(translation_prob[('casa', 'house')], 4/9.)\n",
    "assert_almost_equal(translation_prob[('la', 'house')], 5/18.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wywołaj w pętli 10 kroków algorytmu EM i zaobserwuj jak zmieniają się prawdopodobieństwa dla tłumacznienia \"house\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('casa', 'house'), 0.4884829229547261), (('verde', 'house'), 0.255758538522637), (('la', 'house'), 0.255758538522637)]\n",
      "---\n",
      "[(('casa', 'house'), 0.5457026346909629), (('verde', 'house'), 0.2271486826545185), (('la', 'house'), 0.22714868265451843)]\n",
      "---\n",
      "[(('casa', 'house'), 0.6065598881415178), (('verde', 'house'), 0.19672005592924113), (('la', 'house'), 0.19672005592924105)]\n",
      "---\n",
      "[(('casa', 'house'), 0.6657430854592091), (('verde', 'house'), 0.1671284572703954), (('la', 'house'), 0.16712845727039538)]\n",
      "---\n",
      "[(('casa', 'house'), 0.7204578730894079), (('verde', 'house'), 0.139771063455296), (('la', 'house'), 0.139771063455296)]\n",
      "---\n",
      "[(('casa', 'house'), 0.7693767685977914), (('verde', 'house'), 0.1153116157011043), (('la', 'house'), 0.1153116157011043)]\n",
      "---\n",
      "[(('casa', 'house'), 0.812047157282144), (('verde', 'house'), 0.0939764213589279), (('la', 'house'), 0.0939764213589279)]\n",
      "---\n",
      "[(('casa', 'house'), 0.8485414255151226), (('verde', 'house'), 0.07572928724243876), (('la', 'house'), 0.07572928724243876)]\n",
      "---\n",
      "[(('casa', 'house'), 0.8792386640488387), (('verde', 'house'), 0.06038066797558059), (('la', 'house'), 0.06038066797558059)]\n",
      "---\n",
      "[(('casa', 'house'), 0.9046837882788514), (('verde', 'house'), 0.0476581058605743), (('la', 'house'), 0.0476581058605743)]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    assignment_expected_values = calculate_expectation(english, spanish, translation_prob)\n",
    "    translation_prob = calculate_maximization(english, spanish, assignment_expected_values)\n",
    "    print([(i,j) for i,j in translation_prob.items() if i[1] == \"house\"])\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spróbujmy wykorzystać algorytm EM do stworzenia prawdopodobieństw $t(\\text{angielski}|\\text{polski})$ na poniższym korpusie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "english2 = [[\"the\",\"dog\"], [\"the\",\"house\"], [\"the\", \"green\", \"house\"]]\n",
    "polish = [[\"pies\"], [\"dom\"], [\"zielony\", \"dom\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c323a29363f0022cbf22907ecfcfba25",
     "grade": false,
     "grade_id": "cell-cff38f444b9c8d1a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('the', 'pies'): 0.5, ('dog', 'pies'): 0.5, ('the', 'dom'): 0.4998976014607412, ('house', 'dom'): 0.4998976014607412, ('the', 'zielony'): 0.1681262111738051, ('green', 'zielony'): 0.6637475776523899, ('green', 'dom'): 0.00020479707851748478, ('house', 'zielony'): 0.1681262111738051}\n"
     ]
    }
   ],
   "source": [
    "translation_prob = initalize_translation_prob(set(get_vocabulary(polish)), set(get_vocabulary(english2)))\n",
    "\n",
    "for i in range(10):\n",
    "    assignment_expected_values = calculate_expectation(polish, english2, translation_prob)\n",
    "    translation_prob = calculate_maximization(polish, english2, assignment_expected_values)\n",
    "    \n",
    "print(translation_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdź jak wyglądają prawdopodobieństwa tłumaczeń po 10 iteracjach. \n",
    "\n",
    "UWAGA: wynik algorytmu należy zapisać do zmiennej `translation_prob` -- to ona będzie testowana podczas oceniania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('the', 'pies'): 0.5,\n",
       " ('dog', 'pies'): 0.5,\n",
       " ('the', 'dom'): 0.4998976014607412,\n",
       " ('house', 'dom'): 0.4998976014607412,\n",
       " ('the', 'zielony'): 0.1681262111738051,\n",
       " ('green', 'zielony'): 0.6637475776523899,\n",
       " ('green', 'dom'): 0.00020479707851748478,\n",
       " ('house', 'zielony'): 0.1681262111738051}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bec67383c743edb9dca135c7db9c915d",
     "grade": true,
     "grade_id": "cell-ebbb28e3bee0112e",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_almost_equal\n",
    "# tu są ukryte testy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenia**\n",
    "- Sprawdź czy gdybyś dodał słówko `NULL` to algorytm nauczyłby się wiązać słówko `NULL` z `the`, które nie występuje w języku polskim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "15eaa6a5ef5c9ee228c291a25f163209",
     "grade": true,
     "grade_id": "cell-874a2c94ce1bd996",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('the', 'NULL'): 0.9370242904733792, ('the', 'pies'): 0.09940427160822979, ('dog', 'NULL'): 0.0002794544868163832, ('dog', 'pies'): 0.9005957283917702, ('the', 'dom'): 0.15423765076184237, ('house', 'NULL'): 0.06249885711871718, ('house', 'dom'): 0.8430994831807286, ('the', 'zielony'): 0.023160485778361987, ('green', 'NULL'): 0.00019739792108716355, ('green', 'zielony'): 0.8932057647751056, ('green', 'dom'): 0.002662866057429016, ('house', 'zielony'): 0.08363374944653247}\n"
     ]
    }
   ],
   "source": [
    "english2 = [[\"the\",\"dog\"], [\"the\",\"house\"], [\"the\", \"green\", \"house\"]]\n",
    "polish2 = [[\"NULL\", \"pies\"], [\"NULL\", \"dom\"], [\"NULL\", \"zielony\", \"dom\"]]\n",
    "\n",
    "translation_prob = initalize_translation_prob(set(get_vocabulary(polish2)), set(get_vocabulary(english2)))\n",
    "\n",
    "for i in range(10):\n",
    "    assignment_expected_values = calculate_expectation(polish2, english2, translation_prob)\n",
    "    translation_prob = calculate_maximization(polish2, english2, assignment_expected_values)\n",
    "    \n",
    "print(translation_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('the', 'NULL'): 0.9370242904733792,\n",
       " ('the', 'pies'): 0.09940427160822979,\n",
       " ('dog', 'NULL'): 0.0002794544868163832,\n",
       " ('dog', 'pies'): 0.9005957283917702,\n",
       " ('the', 'dom'): 0.15423765076184237,\n",
       " ('house', 'NULL'): 0.06249885711871718,\n",
       " ('house', 'dom'): 0.8430994831807286,\n",
       " ('the', 'zielony'): 0.023160485778361987,\n",
       " ('green', 'NULL'): 0.00019739792108716355,\n",
       " ('green', 'zielony'): 0.8932057647751056,\n",
       " ('green', 'dom'): 0.002662866057429016,\n",
       " ('house', 'zielony'): 0.08363374944653247}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Jeśli wywołałbyś EM dla pierwszego korpusu równoległego (zmienne `english` i `spanish`) i dołączył tokeny `NULL` to EM tłumaczy NULL jako \"casa\" i \"house\" jako \"casa\" z takimi samymi prawdopodobieństwami. Dlaczego?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0852bf8a51952bbe281c6f8053d2f8b0",
     "grade": true,
     "grade_id": "cell-0982ba49d3a2f657",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Zakładając, że `NULL` przypiszemy do pierwszych słów korpusu, otrzymamy w słowniku 3-krotnie wpis o możliwym tłumaczeniu słowa `casa` na `NULL`. Taka sama liczba występuje też wpisów o tłumaczeniu `casa` na `house`. To powoduje, że otrzymane wartości prawdopodobieństw z algorytmu EM są takie same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 2\n",
    "W tym zadaniu poznasz kolejne elementy biblioteki PyTorch. Konkretnie chodzi o operowanie na wymiarach tensorów, przełącznik `model.training` oraz mnożenie macierzowe paczek danych. Są to trzy oderwane od siebie tematy, ale wszystkie z nich będą potrzebne do implementacji kolejnego zadania.\n",
    "\n",
    "**Przełącznik model.training**\n",
    "Niektóre elementy sieci neuronowych takie jak dropout czy batch normalization inaczej działają w trakcie treningu modelu a inaczej w czasie predykcji. Korzystanie z tych warstw byłoby skomplikowane, gdyż wymagałoby innej obsługi w procedurze `forward`. Z tego powodu każdy z obiektów `nn.Module` posiada właściwość `training`, domyślnie ustawioną na `True`, z której korzystają warstwy takie jak dropout by dostosować swoje działanie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(1,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "model = Model()\n",
    "print( model.training )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przełącznik ten możemy zmieniać przy pomocy funkcji `eval()`, przestawiając model w tryb ewaluacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print( model.training )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A przed treningiem modelu należy wywołać `train()`, przestawiając model w tryb nauki. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "print( model.training )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W dotychczasowych implementacjach modeli nie wykorzystywaliśmy tego przełącznika, gdyż nie korzystaliśmy z warstw których działanie zależało od tego czy dokonujemy predykcji czy ewaluacji. Dobrą praktyką jest jednak zawsze wywoływanie `model.train()` przed treningiem modelu (i odpowiednio `model.eval()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Widoki tensorów**\n",
    "\n",
    "Przy implementacji różnych warstw sieci neuronowych, często konieczne jest inne ułożenie danych w wymiarach tensora tak aby zgadzały się one z formatem wejścia do warstwy kolejnej. Dotychczas wystarczało korzystanie z funkcji `.view()`, która ustawia wymiarowość tensora na tę podaną w jej argumencie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "dane = torch.arange(0,12)\n",
    "print(dane)\n",
    "dane2 = dane.view(3,4)\n",
    "print(dane2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Większość funkcji w Pytorch (które nie są operacjami in-place, końcówka nazwy `_`) zwraca zupełnie nowy tensor. Czy tak jest też tym razem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3, 44,  5,  6,  7,  8,  9, 10, 11])\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [44,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "dane2[1,0] = 44\n",
    "print(dane)\n",
    "print(dane2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zmiana danych w tensorze `dane2` spowodowała automatyczną zmianę danych w tensorze oryginalnym, gdyż funkcja `view()` zwraca jedynie widok do tego samego tensora. Widoki pozwalają na operowanie na tensorach w wygodnym formacie bez konieczności kopiowania danych. Oprócz `.view()` istnieją także inne funkcje, które tworzą widoki tensorów. Poznajmy kilka najpopularniejszych.\n",
    "\n",
    "Funkcja `permute` przyjmuje na wejście permutację kolejnych liczb naturalnych od 0 do wymiarowości tensora i układa wymiary w ten właśnie sposób. Dla przykładu `dane2.permute(0,1)` zwróci identyczny widok, gdyż jako pierwszy wymiar ustawi wymiar 0, a jako drugi wymiar 1. Jednak już `dane2.permute(1,0)` dokona transpozycji macierzy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [44,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[ 0, 44,  8],\n",
      "        [ 1,  5,  9],\n",
      "        [ 2,  6, 10],\n",
      "        [ 3,  7, 11]])\n"
     ]
    }
   ],
   "source": [
    "print ( dane2.permute(0,1) )\n",
    "print ( dane2.permute(1,0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogicznie działa to z większą liczbą wymiarów, w szczególności z trzema. Jeśli więc mamy ułożone dane w postaci `[długość sekwencji, wielkość paczki danych, liczba cech]` i chcemy je ułożyć tak by indeksowanie po poszczególych elementach paczki było na pierwszym wymiarze wystarczy stworzyć odpowiedni widok poprzez `permute`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1],\n",
      "         [ 2,  3]],\n",
      "\n",
      "        [[44,  5],\n",
      "         [ 6,  7]],\n",
      "\n",
      "        [[ 8,  9],\n",
      "         [10, 11]]]) torch.Size([3, 2, 2])\n",
      "tensor([[[ 0,  1],\n",
      "         [44,  5],\n",
      "         [ 8,  9]],\n",
      "\n",
      "        [[ 2,  3],\n",
      "         [ 6,  7],\n",
      "         [10, 11]]]) torch.Size([2, 3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dane3 = dane.view(3,2,2)\n",
    "print(dane3, dane3.shape)\n",
    "dane3p = dane3.permute(1,0,2)\n",
    "print(dane3p, dane3p.shape)\n",
    "# Elementy są identyczne po odpowiednim spermutowaniu indeksów\n",
    "dane3[2,1,0] == dane3p[1,2,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kolejną przydatną funkcją tworzącą widoki jest `unsqueeze` (i `squeeze`), która dodaje dodatkowy wymiar (o rozmiarze 1) w odpowiednim miejscu. Przykładowo wektor dane, chcielibyśmy przetworzyć jakąś warstwą, ale tak by potraktowała to jako 1-elementa paczka elementów o wymiarowości 12 (macierz 1x12). W tym celu możemy dodać sztuczny wymiar na pierwszej pozycji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3, 44,  5,  6,  7,  8,  9, 10, 11])\n",
      "tensor([[ 0,  1,  2,  3, 44,  5,  6,  7,  8,  9, 10, 11]])\n",
      "torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "daneu = dane.unsqueeze(0)\n",
    "print(dane)\n",
    "print(daneu)\n",
    "print(daneu.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogicznie wykorzystaj funkcję `unsqueeze` aby uzyskać macierz 12x1 z wektora `dane`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a15f68a9c03e74fa4a025e7b13d05cef",
     "grade": true,
     "grade_id": "cell-32537683a38f635f",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3, 44,  5,  6,  7,  8,  9, 10, 11])\n",
      "tensor([[ 0],\n",
      "        [ 1],\n",
      "        [ 2],\n",
      "        [ 3],\n",
      "        [44],\n",
      "        [ 5],\n",
      "        [ 6],\n",
      "        [ 7],\n",
      "        [ 8],\n",
      "        [ 9],\n",
      "        [10],\n",
      "        [11]])\n",
      "torch.Size([12, 1])\n"
     ]
    }
   ],
   "source": [
    "daneu2 = dane.unsqueeze(1)\n",
    "print(dane)\n",
    "print(daneu2)\n",
    "print(daneu2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z korzystaniem z widoków mogą się jednak wiązać problemy wydajnościowe modeli uczenia maszynowego. Wiele intensywnych obliczeniowo operacji matematycznych można efektywnie zaimplementować wykorzystując lokalność danych tj. zakładając że kolejne dane tensora/macierzy/wektora ułożone są obok siebie w pamięci. Wyobraź sobie np. warstwę liniową odczytującą cechy przykładu uczącego które przylegają do siebie w pamięci, a przykładu którego kolejne cechy, wskutek permutacji wymiarów, leżą w odległych miejscach pamięci. \n",
    "\n",
    "W PyTroch możemy sprawdzić czy tensor zawiera dane, które przylegają do siebie w pamięci wywołując funkcję `is_contiguous()` (konkretnie dane powinny być ułożone w pamięci po kolei od skrajnie prawego wymiaru tj. w przypadku macierzy wierszowo). Jest też możliwe wywołanie funkcji `contiguous()`, która wymusza przyległe ułożenie danych w tensorze poprzez skopiowanie danych i stworzenie nowego tensora. Pomimo kosztu pamięciowego i czasowego, przed wykonaniem szczególnie kosztownych operacji obliczeniowych, wykonanie tej operacji może się opłacać. Można też uważnie operować na tensorach, by unikać nie-przylegających tensorów ;) Zwróć uwagę, że nie każdy widok od razu powoduje brak przyległości danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(daneu.is_contiguous())\n",
    "print(dane3p.is_contiguous())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mnożenie macierzowe z paczkami danych**\n",
    "\n",
    "Na koniec poznajmy przydatną funkcję `bmm` czyli *batch matrix multiplication*. Rozważmy sytuację w której mamy kolekcję 3 macierzy (a1, a2, a3) i potrzebujemy uzyskać wyniki mnożenia tych macierzy przez analogiczną kolekcję macierzy (b1, b2, b3). Możemy wykonać te mnożenia kolejno w pętli tj. a1@b1, a2@b2 i a3@b3, jednak istnieje także inna możliwość: spakować te macierze do tensorów i wykonać operację `bbm`, która zwróci nam dokładnie taki sam wynik (ale znacznie szybciej niż w przypadku iterowania)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[[1,0],[0,1]],  [[2,0],[0,2]],  [[1,1],[1,1]] ])\n",
    "B = torch.tensor([[[1,2],[3,4]],  [[-1,-2],[-3,-4]], [[1,2],[3,4]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1,  2],\n",
      "         [ 3,  4]],\n",
      "\n",
      "        [[-2, -4],\n",
      "         [-6, -8]],\n",
      "\n",
      "        [[ 4,  6],\n",
      "         [ 4,  6]]])\n",
      "A torch.Size([3, 2, 2]), B torch.Size([3, 2, 2]), result torch.Size([3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "result = torch.bmm(A,B)\n",
    "print(result)\n",
    "print(f\"A {A.shape}, B {B.shape}, result {result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 3\n",
    "Celem tego zadania jest zaimplementowanie prostej architektury koder-dekoder dla problemu tłumaczenia maszynowego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "data = Multi30k(split='train', language_pair=('de', 'en'))\n",
    "data_val = Multi30k(split='valid', language_pair=('de', 'en'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konieczna jest instalacja tokenizatorów dla angielskiego i niemieckiego (a wcześniej biblioteki `spacy` jeśli nie masz jej zainstalowanej)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting de-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\python39\\lib\\site-packages (from de-core-news-sm==3.7.0) (3.7.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.28.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.2)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.64.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: setuptools in c:\\users\\marcin\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (65.5.0)\n",
      "Requirement already satisfied: jinja2 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.19.0; python_version >= \"3.9\" in c:\\users\\marcin\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (21.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.10.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2022.6.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python39\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.3.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\python39\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\python39\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\python39\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\python39\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\python39\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python39\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\python39\\lib\\site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.7.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\python39\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\marcin\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.5.0)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: numpy>=1.19.0; python_version >= \"3.9\" in c:\\users\\marcin\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.23.5)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.28.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: jinja2 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.64.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\python39\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: colorama>=0.4.6; sys_platform == \"win32\" and python_version >= \"3.7\" in c:\\python39\\lib\\site-packages (from wasabi<1.2.0,>=0.9.1->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\python39\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\python39\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\python39\\lib\\site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\python39\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.3.0)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\python39\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.11)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\python39\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python39\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download de_core_news_sm\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klasa `ParallelCorpus` jest typu `Dataset` co pozwala na jej użycie w obiektach klasy `DataLoader`. Do jej konstruktora należy podać zbiór danych, dwa kody języków (źródło, cel) oraz (opcjonalnie) dwa obiekty typu `Vocab` i liczbę `data_limit`. Jeżeli obiekty `Vocab` nie zostaną podane to obiekt `ParallelCorpus` utworzy je automatycznie na podstawie zbioru danych. Jeśli zaś nie podasz `data_limit` to zostanie wczytany cały zbiór danych, w przeciwnym razie wczytanie zostanie tylko `data_limit` par zdań. W dalszej implementacji będziesz potrzebował odczytać wielkość słownika co możesz zrobić poprzez wywołanie `len()` na obiektach `vocab_a` i `vocab_b`, które są właściwością klasy `ParallelCorpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\torchtext\\data\\utils.py:105: UserWarning: Spacy model \"de\" could not be loaded, trying \"de_core_news_sm\" instead\n",
      "  warnings.warn(\n",
      "c:\\Python39\\lib\\site-packages\\torchtext\\data\\utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from helpers import ParallelCorpus\n",
    "\n",
    "dataset = ParallelCorpus(data,'de', 'en', data_limit = 10000)\n",
    "#W zbiorze walidacyjnym używamy tych samych słowników\n",
    "dataset_val = ParallelCorpus(data_val, 'de', 'en', dataset.vocab_a, dataset.vocab_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponieważ w porównaniu do poprzednich zadań obliczenia mogą być bardziej czasochłonne to możesz skorzystać z możliwości karty graficznej zgodnej z CUDA (jeśli taką posiadasz). Aby obliczenia odbywały się na karcie graficznej wszystkie tensory uczestniczące w operacji (np. parametry modelu oraz dane) muszą zostać w niej zapisane. Możesz to zrobić wywołując na modelu lub tensorze funkcję `.to(device)` której argumentem jest urządzenie (cpu lub karta graficzna). Poniższa linijka pozwala na sprawdzenie czy karta graficzna zgodna z CUDA jest dostępna oraz przypisanie odpowiedniego urządzenia. \n",
    "\n",
    "*UWAGA* Jeśli nie jesteś w posiadaniu karty graficznej z CUDA to możesz mocno zmniejszyć `data_limit`, a z kolei jeśli ją masz to możesz go podwyższyć. Ilość danych wpływa na jakość uzyskanych wyników, jednak ocenie podlega poprawność implementacji, a nie jakość wyników. W przypadku dalszych problemów możesz także zmniejszyć pojawiające się dalej stałe `HIDDEN_DIM` i `WORD_EMBEDDING` czy wielkość paczki danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Koder w naszej implementacji to dwukierunkowa sieć rekurencyjna typu GRU, do której wejściem są kolejne tokeny przetworzone przez macierz zanurzeń."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_EMBEDDING = 128\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, WORD_EMBEDDING)\n",
    "        self.rnn = nn.GRU(WORD_EMBEDDING, hidden_size // 2 , bidirectional = True)\n",
    "        \n",
    "    def forward(self, src, src_len):\n",
    "        embedded = self.embedding(src)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len)\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)         \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
    "        hidden = torch.cat([hidden[0,:, :], hidden[1,:,:]], dim=1)\n",
    "        return outputs, hidden.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogicznie, dekoder jest jednokierunkową siecią rekurencyjną typu GRU o takiej samej wymiarowości stanu ukrytego co enkoder. Z tego powodu stan ukryty kodera może zostać przekazany dekoderowi bez żadnej transformacji.\n",
    "\n",
    "Dekoder jest jednak trochę bardziej skomplikowany, gdyż w każdej iteracji na wejście otrzymuje kolejne słowo, które zostało przewidziane (!) w poprzedniej iteracji. Nie jest więc możliwe skolekcjonowanie całej sekwencji wejściowej i jednorazowe wywołanie `rnn(wejście)` tylko trzeba to robić element po elemencie. Dodatkowo wejściem do dekodera jest też wektor kontekstu, który powinien zostać stworzony przez mechanizm uwagi (obecnie nie jest on zaimplementowany).\n",
    "\n",
    "Podsumowując: wejściem do dekodera jest słowo z poprzedniej iteracji (przetworzone przez macierz zanurzeń), wektor kontekstu oraz stan ukryty dekodera z ostatniej iteracji (oprócz pierwszej w której wejściem jest ostatni stan kodera).\n",
    "\n",
    "Wyjście dekodera to z kolei przetłumaczone słowo, które jest w tej implementacji przewidywane na podstawie warstwy liniowej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        # Słowo z poprzedniej iteracji jest reprezentowane zanurzeniem \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, WORD_EMBEDDING)\n",
    "        self.rnn = nn.GRU(hidden_size + WORD_EMBEDDING, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, input, hidden, encoder_outputs, src_len):\n",
    "        # Funckję wywołujemy iteracyjnie tzn. tylko dla jednego słowa \n",
    "        # dla każdej z paczki przetwarzanych sekwencji\n",
    "        \n",
    "        # input = [batch size] (indeks tylko jednego słowa per zdanie)\n",
    "        # hidden = [batch size, hidden_size]\n",
    "        # encoder_outputs = [src len, batch size, hidden_size]\n",
    "        # mask = [batch size, src len]\n",
    "      \n",
    "        embedded = self.embedding(input)\n",
    "        \n",
    "        #Wektor zer - w przyszłości wynik mechanizmu uwagi\n",
    "        # context = [batch size, hidden_size]\n",
    "        context = torch.zeros((input.shape[0], self.rnn.input_size - WORD_EMBEDDING)).to(embedded.device)\n",
    "        \n",
    "        #Wejście do sieci do wektor kontekstu połączony z przetwarzanym słowem\n",
    "        rnn_input = torch.cat((embedded, context), dim = 1)\n",
    "        \n",
    "        # Wejście do sieci rekurencyjnej zawiera wymiar - długość przetwarzanej sekwencji\n",
    "        # Konieczne jest więc \"sztuczne\" dodanie wymiaru na pierwszej (tj. zerowej) pozycji\n",
    "        rnn_input = rnn_input.unsqueeze(0)\n",
    "        #rnn_input = [1, batch size, word_embedding + hidden_size]\n",
    "        \n",
    "        _, hidden = self.rnn(rnn_input, hidden)\n",
    "        \n",
    "        prediction = self.fc(hidden.squeeze(0))\n",
    "        \n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pytania sprawdzające zrozumienie implementacji**\n",
    "- Dlaczego podczas zwracania stanu ukrytego kodera musieliśmy połączyć jego dwa pierwsze wymiary, aby zainicjalizować nimi dekoder?\n",
    "- W implementacji kodera osobno zwróciliśmy `outputs`, oraz `hidden`. To ostatenie jeszcze musieliśmy połączyć w dłuższe wektory aby uzyskać jeden wektor stanu ukrytego dla każdego elementu paczki danych (gdyż sieć była dwukierunkowa). Czy nie prościej byłoby wziąć `outputs[-1]`? Podpowiedź: dla każdego $i>0$ `outputs[-1][i]` zawiera same zera.\n",
    "- W implementacji dekodera do warstwy liniowej przekazaliśmy `hidden.squeeze(0)` tj. usunęliśmy pierwszy wymiar tensora (który miał długość 1). Czy jest to wymiar związany z długością przetwarzanej sekwencji przez sieć rekurencyjną, który sztucznie dodaliśmy kilka linijek wyżej? Podpowiedź: nie, sprawdź  dokumentację neuronu GRU w Pytorch\n",
    "- Czy macierz zanurzeń dekodera i enkodera mogłaby być współdzielona? Tj. będzie zawierała reprezentacje tych samych słów?\n",
    "\n",
    "Ostatecznie implementujemy model koder-dekoder, który łączy dwa powyższe obiekty. W szczególności ważna jest funkcja `forward`, która iteracyjnie wywołuje dekoder dla kolejno przetwarzanych słów. Bardziej konkretnie: dla poprzedniego słowa w języku docelowym. W tym miejscu mamy do wyboru dwie stragie: pierwsza to podawanie na wejście dekodera najbardziej prawdopodobne słowo które poprzednio przewidział lub zastosowanie *teacher forcing*. W tej implementacji model znajdujący się w trybie ewaluacji będzie przekazywał dekoderowi przewidziane przez niego słowa, a w fazie treningu będzie korzystał z *teacher forcing*.\n",
    "\n",
    "Zwróć uwagę, że model zwraca całą serię wyjść z dekodera (zmienna `outputs`) - jest to konieczne by policzyć funkcję straty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = next(enc.parameters()).device\n",
    "        \n",
    "    def forward(self, src, src_len, tgt):       \n",
    "        tgt_paded_len = tgt.shape[0]\n",
    "        batch_size = tgt.shape[1]\n",
    "        vocab_tgt_size = self.decoder.vocab_size\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(tgt_paded_len, batch_size, vocab_tgt_size).to(self.device)\n",
    "        \n",
    "        # Przetworzenie wejścia przez koder, kolejno uzyskane reprezentacje\n",
    "        # kazdego słowa będą potrzebne do implementacji mechanizmu uwagi\n",
    "        # Z kolei hidden posłuży do inicjalizacji stanu ukrytego dekodera\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        prev_word = tgt[0,:]\n",
    "                \n",
    "        for i in range(1, tgt_paded_len):          \n",
    "            output, hidden = self.decoder(prev_word, hidden, encoder_outputs, src_len)\n",
    "            outputs[i] = output\n",
    "               \n",
    "            # uczenie poprzez teaching forcing\n",
    "            prev_word = tgt[i] if self.training else output.argmax(1)\n",
    "            \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ostatecznie tworzymy obiekty typu `DataLoader` z odpowiednia funkcją tworzącą paczkę danych (nie musisz jej analizować)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for sample in batch:\n",
    "        src_batch.append(sample[\"text_a\"])\n",
    "        tgt_batch.append(sample[\"text_b\"])\n",
    "    src_batch = pad_sequence(src_batch, padding_value=0)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=0)\n",
    "    lena_batch = torch.tensor([len(sample[\"text_a\"]) for sample in batch], dtype=torch.int64)\n",
    "    lenb_batch = torch.tensor([len(sample[\"text_b\"]) for sample in batch], dtype=torch.int64)\n",
    "    idx = torch.argsort(lena_batch, descending=True)\n",
    "    return src_batch[:,idx].to(device), tgt_batch[:,idx].to(device), lena_batch[idx], lenb_batch[idx]\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=64, collate_fn=collate_fn)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=256, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A także pętlę uczącą model, zwróć uwagę na ustawienie przełącznika `.train()`, stosowanie techniki normalizacji gradientu, ignorowanie w funkcji celu etykiet `<pad>` (uzupełnienie sekwencji do wspólnej długości) oraz na pominięcie pierwszego tokenu `<start>` w sekwencji docelowej. Pomimo tego, że w zbiorze danych każde zdanie zaczyna się od tokenu `<start>` i kończy się tokenem `<stop>`, i w związku z tym token `<start>` trafia na wejście dekodera w pierwszej iteracji, to dekoder od razu przewiduje kolejne słowo tłumaczonego zdania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Loss: 4.588\n",
      "Epoch: 02 | Loss: 3.512\n",
      "Epoch: 03 | Loss: 3.109\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_DIM = 256\n",
    "EPOCHS = 3\n",
    "\n",
    "enc = Encoder(len(dataset.vocab_a), HIDDEN_DIM).to(device)\n",
    "dec = Decoder(len(dataset.vocab_b), HIDDEN_DIM).to(device)\n",
    "enc_dec = EncoderDecoder(enc, dec).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(enc_dec.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "\n",
    "enc_dec.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        src, tgt, src_len, tgt_len = batch\n",
    "        \n",
    "        outputs = enc_dec(src, src_len, tgt)\n",
    "\n",
    "        # Token START nie jest przewidywany przez dekoder\n",
    "        tgt = tgt[1:].view(-1)\n",
    "        \n",
    "        # Tablekę outputs zaczęliśmy uzupełniać od indeksu 1\n",
    "        outputs = outputs[1:].view(-1, dec.vocab_size)\n",
    "\n",
    "        loss = criterion(outputs, tgt)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Ucinanie gradientu\n",
    "        torch.nn.utils.clip_grad_norm_(enc_dec.parameters(), 2.)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        epoch_loss += loss   \n",
    "    print(f'Epoch: {epoch+1:02} | Loss: {epoch_loss / len(dataloader):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykonanie powyższych 3 epok uczenia może zająć na szybkim CPU ok. 1,5 minuty.\n",
    "\n",
    "Sprawdź jak radzi sobie model z przetłumaczeniem przykładowego zdania ze zbioru uczącego (oczywiście, wynik może być  daleki od optymalnego z powodu krótkiego czasu treningu i pomniejszonego zbioru danych). Następnie sprawdź jak sobie model radzi z tłumaczeniem wybranego zdania ze zbioru walidacyjnego (obiekt `dataset_val`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Żródło = ['<start>', 'Ein', 'schwarzer', 'Hund', 'und', 'ein', 'gefleckter', 'Hund', 'kämpfen', '.', '<stop>']\n",
      "Tłumaczenie = ['<start>', 'A', 'black', 'dog', 'is', 'running', 'through', 'a', 'field', '.', '<stop>']\n",
      "Referencja = ['<start>', 'A', 'black', 'dog', 'and', 'a', 'spotted', 'dog', 'are', 'fighting', '<stop>']\n"
     ]
    }
   ],
   "source": [
    "from helpers import translate\n",
    "example_idx = 12\n",
    "\n",
    "src = dataset[example_idx][\"text_a\"]\n",
    "translate(src, enc_dec, dataset, device)\n",
    "\n",
    "tgt =  dataset[example_idx][\"text_b\"]\n",
    "print(f'Referencja = {dataset.vocab_b.lookup_tokens(tgt.numpy())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Zaimplemenuj funkcję, która policzy średnią wartość entropii krzyżowej na zbiorze walidacyjnym, która byłaby przydatna np. do wybrania odpowiedniej liczby epok uczących.\n",
    "- pamiętaj o przełączeniu modelu w tryb ewaluacji\n",
    "- pamiętaj o nieśledzeniu wartości gradientów `with torch.no_grad()`\n",
    "- wykorzystaj już wcześniej zainicjalizowany `dataloader_val`, który iteruje po paczkach zbioru walidacyjnego\n",
    "- na koniec dopisz wywołanie twojej funkcji do powyższej pętli uczącej algorytm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e942d80c24b7c174b65a100e73c800be",
     "grade": true,
     "grade_id": "cell-bf4c15791c51a28f",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.617352843284607\n"
     ]
    }
   ],
   "source": [
    "def evaluate_validation_set(model, dataloader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            src, tgt, src_len, tgt_len = batch\n",
    "\n",
    "            outputs = model(src, src_len, tgt)\n",
    "            tgt = tgt[1:].view(-1)\n",
    "            outputs = outputs[1:].view(-1, model.decoder.vocab_size)\n",
    "\n",
    "            loss = criterion(outputs, tgt)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "loss = evaluate_validation_set(enc_dec, dataloader_val, criterion)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozbuduj dekoder o mechanizm uwagi (tj. o obliczanie wektora `context`) przedstawiony na wykładzie, gdzie do policzenia wag uwagi jest wykorzystany zwykły iloczyn skalarny. Stan ukryty dekodera `hidden` wykorzystaj jako wektor `q`, a stany ukryte kodera dla każdego elementu zdania wejściowego są w tensorze `encoder_outputs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "add31f3b44b55ac1b0a74bbf8473405d",
     "grade": true,
     "grade_id": "cell-257a4a58e55953f8",
     "locked": false,
     "points": 14,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        # Słowo z poprzedniej iteracji jest reprezentowane zanurzeniem \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, WORD_EMBEDDING)\n",
    "        self.rnn = nn.GRU(hidden_size + WORD_EMBEDDING, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, input, hidden, encoder_outputs, src_len):\n",
    "        # Funckję wywołujemy iteracyjnie tzn. tylko dla jednego słowa \n",
    "        # dla każdej z paczki przetwarzanych sekwencji\n",
    "        \n",
    "        # input = [batch size] (indeks tylko jednego słowa per zdanie)\n",
    "        # hidden = [batch size, hidden_size]\n",
    "        # encoder_outputs = [src len, batch size, hidden_size]\n",
    "        # mask = [batch size, src len]\n",
    "      \n",
    "        embedded = self.embedding(input)\n",
    "        \n",
    "        h_att = hidden.permute(1,0,2)\n",
    "        eo_att = encoder_outputs.permute(1,2,0)\n",
    "\n",
    "        att = torch.bmm(h_att, eo_att)\n",
    "        att = att.softmax(2)\n",
    "\n",
    "        eo_ctx = encoder_outputs.permute(1,0,2)\n",
    "        context = torch.bmm(att, eo_ctx)\n",
    "        context = context.squeeze(1)\n",
    "        \n",
    "        # context = [batch size, hidden_size]\n",
    "        \n",
    "        #Wejście do sieci do wektor kontekstu połączony z przetwarzanym słowem\n",
    "        rnn_input = torch.cat((embedded, context), dim = 1)\n",
    "        \n",
    "        # Wejście do sieci rekurencyjnej zawiera wymiar - długość przetwarzanej sekwencji\n",
    "        # Konieczne jest więc \"sztuczne\" dodanie wymiaru na pierwszej (tj. zerowej) pozycji\n",
    "        rnn_input = rnn_input.unsqueeze(0)\n",
    "        #rnn_input = [1, batch size, word_embedding + hidden_size]\n",
    "        \n",
    "        _, hidden = self.rnn(rnn_input, hidden)\n",
    "        \n",
    "        prediction = self.fc(hidden.squeeze(0))\n",
    "        \n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenia**\n",
    "- Porównaj wyniki uzyskane modelem z mechanizmem uwagi z wynikami bez niego. Czy udało się osiągnąć lepszą jakość tłumaczenia lub unika się pewnych rodzajów błędów?\n",
    "- Jak wpływa dodanie mechanizmu uwagi na dynamikę uczenia się? Trwa ono dużej/krócej? Funkcja celu spada szybciej czy wolniej?\n",
    "- Jakie czynniki są według Ciebie kluczowe jeśli chodzi o szybkość treningu systemów NMT a także wymagania pamięciowe dla nich?\n",
    "\n",
    "Odpowedzi na powyższe nie musisz wpisywać."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Loss: 4.586\n",
      "Epoch: 02 | Loss: 3.569\n",
      "Epoch: 03 | Loss: 3.139\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_DIM = 256\n",
    "EPOCHS = 3\n",
    "\n",
    "enc = Encoder(len(dataset.vocab_a), HIDDEN_DIM).to(device)\n",
    "dec = DecoderWithAttention(len(dataset.vocab_b), HIDDEN_DIM).to(device)\n",
    "enc_dec = EncoderDecoder(enc, dec).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(enc_dec.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "\n",
    "enc_dec.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        \n",
    "        src, tgt, src_len, tgt_len = batch\n",
    "        \n",
    "        outputs = enc_dec(src, src_len, tgt)\n",
    "\n",
    "        # Token START nie jest przewidywany przez dekoder\n",
    "        tgt = tgt[1:].view(-1)\n",
    "        \n",
    "        # Tablekę outputs zaczęliśmy uzupełniać od indeksu 1\n",
    "        outputs = outputs[1:].view(-1, dec.vocab_size)\n",
    "\n",
    "        loss = criterion(outputs, tgt)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Ucinanie gradientu\n",
    "        torch.nn.utils.clip_grad_norm_(enc_dec.parameters(), 2.)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        epoch_loss += loss   \n",
    "    print(f'Epoch: {epoch+1:02} | Loss: {epoch_loss / len(dataloader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
