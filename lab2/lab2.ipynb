{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten notebook jest oceniany półautomatycznie. Nie twórz ani nie usuwaj komórek - struktura notebooka musi zostać zachowana. Odpowiedź wypełnij tam gdzie jest na to wskazane miejsce - odpowiedzi w innych miejscach nie będą sprawdzane (nie są widoczne dla sprawdzającego w systemie).\n",
    "\n",
    "W szczególności zwróć uwagę, że usupełniłeś wszystkie miejsca `YOUR CODE HERE`, `WPISZ TWÓJ KOD TUTAJ`, \"YOUR ANSWER HERE\" lub \"WPISZ TWOJĄ ODPOWIEDŹ TUTAJ\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zaawansowane Przetwarzanie Języka Naturalnego\n",
    "# Laboratorium 2\n",
    "\n",
    "Pobierz zbiór danych Amazon \"Musical Instruments\" z [tej](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Musical_Instruments_5.json.gz) strony internetowej, a następnie wczytaj go poniższym kodem. Zwróć uwagę na wymaganą lokalizację pliku, tj. dwa katalogi wyżej - wynika to ze struktury plików w sprawdzarce, przepraszam za niedogodność.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    " \n",
    "x_text = []\n",
    "y = []\n",
    "with open('../../Musical_Instruments_5.json') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        x_text.append(data['reviewText'].lower().strip())\n",
    "        y.append(int(data['overall']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1 - przygotowanie danych\n",
    "W załadowanych listach `x_text` oraz `y` znajdują się odpowiednio teksty kolejnych opinii oraz oznaczenia klas. Klasą w tym wypadku jest liczba gwiazdek (ocena) produktu towarzysząca opinii. Zadanie klasyfikacji polega na przewidzeniu oceny na podstawie opinii pozostawionej w portalu.\n",
    "\n",
    "Aby zmniejszyć wymagania obliczeniowe do dalszych eksperymentów, ograniczymy zbiór danych jedynie do pierwszego tysiąca opinii.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_text = x_text[:1000]\n",
    "y = y[:1000]\n",
    "train_end_idx=int(0.9 * len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jedną z użytecznych operacji przygotowania tekstu do konstrukcji klasyfikatora jest zastąpienie poszczególnych tokenów ich indeksami. Chociaż w praktyce ten proces często następuje dopiero po szeregu etapów przetwarzania tekstu takich jak tokenizacja, lematyzacja czy stemming - w tym ćwiczeniu wyodrębnimy tokeny rozdzielając tekst znakiem spacji.\n",
    "\n",
    "Klasyfikator powinien obsługiwać także słowa, które nie występowały w zbiorze uczącym. Podstawową techniką obsługi takich słów jest wprowadzenie specjalnego tokenu UNK, obsługującego nieznane słowa. W tym celu usuwa się ze zbioru danych pewną liczbę najrzadszych słów i zastępuje się je tokenami UNK.\n",
    "\n",
    "Zbuduj słownik `w2i` mapujący tokeny na kolejne indeksy tj. liczby naturalne. Pomiń tokeny występujące w zbiorze uczącym 5 lub mniej razy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f866dd9918a27f9ba2f31ef62a061fa1",
     "grade": false,
     "grade_id": "cell-87edb8b6e5279a0e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "UNK = w2i[\"<unk>\"] #Przypisz indeks tokenowi UNK\n",
    "\n",
    "words = list(chain(*[sentence.split(\" \") for sentence in x_text]))\n",
    "counter = Counter(words)\n",
    "for token in words:\n",
    "    if token not in w2i.keys() and counter[token] > 5:\n",
    "        w2i[token]\n",
    "        \n",
    "\n",
    "n_words = len(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po zbudowaniu słownika `w2i`, przekonwertujmy nasz zbiór danych z listy słów na listę indeksów słów. Od razu podzielimy zbiór na część uczącą i część testową, a także przekonwertujemy klasy na indeksy klas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = defaultdict(lambda: UNK, w2i) # Domyślną wartością słownika jest UNK, \n",
    "         #chociaż w2i będzie zawierał wpisy do wszystkich słów to nowym tokenom będzie przypisywał indeks UNK\n",
    "class2i = defaultdict(lambda: len(class2i))\n",
    "        # mapuj klasy na indeksy klas\n",
    "    \n",
    "def read_dataset(start_idx,end_idx):\n",
    "    for i, text in enumerate(x_text[start_idx:end_idx]):\n",
    "        yield ([w2i[x] for x in text.split(\" \")], class2i[y[i]])\n",
    "        \n",
    "train = list(read_dataset(0, train_end_idx))\n",
    "dev = list(read_dataset(train_end_idx, len(y)))\n",
    "n_class = len(class2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1389 5\n"
     ]
    }
   ],
   "source": [
    "print(n_words, n_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 2 - pierwszy model klasyfikacji tekstu w PyTorch\n",
    "Podstawową strukturą danych w PyTorch jest tensor, na którym możesz wykonywać analogiczne operacje jak na macierzach `numpy`. Podstawową metodą stworzenia tensora jest wywołanie konstruktora `torch.tensor` na liście liczb. Istnieją także inne konstruktory tensorów, analogiczne do `numpy`. Można też na nich operować za pomocą standardowych operatorów, indeksowania, i odpowiedników innych funkcji znanych z `numpy`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([[0.5711, 0.9193, 0.3689],\n",
      "        [0.4934, 0.0594, 0.3880],\n",
      "        [0.1660, 0.3467, 0.7812]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor([1,2,3]))\n",
    "print(torch.rand( (3,3) ))\n",
    "print(torch.ones( (3,3) ))\n",
    "print(2 * torch.ones( (3,3) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dlaczego więc korzystamy z PyTorch, a nie z biblioteki `numpy` skoro tensory wydają się mieć analogiczną funkcjonalność do poznanych uprzednio macierzy? Powodów jest oczywiście wiele, m.in. możliwość przeniesienia obliczeń na kartę graficzną (technologia CUDA), ale z punktu widzenia naszego ćwiczenia kluczowa jest funkcjonalność automatycznego liczenia gradientów. W przypadku konstrukcji sieci neuronowej czy modelu liniowego, w PyTorch nie jest konieczne samodzielne wyprowadzanie i implementowanie gradientu, gdyż biblioteka zrobi to za nas automatycznie.\n",
    "\n",
    "Wyznaczanie gradientów odbywa się za pomocą algorytmu wstecznej propagacji, który ma dwie fazy: *forward* i *backward*. Faza *forward* polega na policzeniu wyniku funkcji, a faza *backward* wyznacza gradienty wszystkich jej parametrów.\n",
    "\n",
    "W celu poznania tej funkcjonalności policzymy pochodne cząstkowe prostej funkcji kwadratowej:\n",
    "$$result = x_1^2 + x_2^2+ x_3^2$$\n",
    "której pochodne cząstkowe mają postać $2x_i$.\n",
    "\n",
    "Rozpocznijmy implementacje tej funkcji od stworzenia 3-elementowego wektora zmiennych `x`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.,2.,3.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak pewnie zauważyłeś, w konstruktorze użyliśmy dodatkowego parametru `requires_grad`. Domyślnie wykonanie operacji na dowolnym tensorze nie traktuje się jako części fazy `forward`, gdyż nie do wszystkich tensorów użytych w kodzie będziemy potrzebować wartości pochodnych. Aby zasygnalizować, że dla danej zmiennej konieczne jest zapisywanie informacji o wykonywanych na niej operacjach, należy ustawić wartość jej parametru `requires_grad` na `True`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przejdźmy do policzenia wartości wyżej zdefiniowanej funkcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (x**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensory posiadają parametr `.grad`, który przechowuje informacje o wyznaczonym gradiencie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tej chwili, pomimo obliczenia wartości zmiennej `result`, wartość gradientu nie jest policzona, gdyż nie poinformowaliśmy biblioteki o zakończeniu fazy `forward` i konieczności wykonania fazy `backward`. Możemy to zrobić poprzez wykonanie funkcji `backward()` na obliczonej wartości funkcji (funkcję tę można wywołać tylko na skalarnym wyniku!).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwróć uwagę, że wektor `x.grad`, zgodnie z naszymi oczekiwaniami, zawiera wartości pochodnej cząstkowej tj. `2x`. Spróbujmy jeszcze raz, licząc pochodną po logarytmie z `result`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = torch.log(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32me:\\Dokumenty\\Na studia\\Semestr 9\\Zaawansowane Przetwarzanie Języka Naturalnego\\lab2\\lab2.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Dokumenty/Na%20studia/Semestr%209/Zaawansowane%20Przetwarzanie%20J%C4%99zyka%20Naturalnego/lab2/lab2.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m result2\u001b[39m.\u001b[39;49mbackward()\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "result2.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Niestety operacja się nie powiodła. Przed wykonaniem kolejnej fazy *backward* należy - upraszczając - wykonać fazę *forward*. Nasze poprzednie operacje konstruowały fazę *forward* od parametrów `x` aż do zmiennej z wynikiem, jednakże przy wykonaniu fazy *backward* została zwolniona pamięć przechowująca informacje o kolejno wykonywanych operacjach na tych zmiennych (graf obliczeń). Kolejna operacja została wykonana bezpośrednio na tensorze `result`, konstruując fazę *forward* od `result` do `result2`, jednak zabrakło grafu obliczeń od parametrów `x`.\n",
    "\n",
    "Uruchomienie poniższego kodu, z operacjami rozpoczynającymi się od `x`, zakończy się obliczeniem gradientu z sukcesem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.1429, 4.2857, 6.4286])\n"
     ]
    }
   ],
   "source": [
    "result = (x**2).sum()\n",
    "result2 = torch.log(result)\n",
    "result2.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy poprawność uzyskanego wyniku. Zmienna $result= 1^2+2^2+3^2=14$, a pochodna z logarytmu naturalnego to $\\frac{1}{x}$. W związku z tym:\n",
    "$$\\frac{\\partial }{\\partial x_1} \\log result = \\frac{1}{result} \\cdot \\frac{\\partial }{\\partial x_1} result = \\frac{1}{result} 2x_1 $$\n",
    "Przy naszych wartościach $x$ równa się to $\\frac{1}{14}\\cdot 2 = 0,1428$. Łatwo zauważyć, że wynik znajdujący się w tensorze `x.grad` jest błędny, a konkretnie za duży o 2 jednostki.\n",
    "\n",
    "Stało się tak dlatego, że gradient z kolejnych faz `backward` jest akumulowany w parametrze `.grad` (poprzednia wartość policzonej pochodnej cząstkowej wynosiła właśnie 2). Takie zachowanie biblioteki może być bardzo użyteczne w sytuacji gdy chcemy w zmiennej zagregować gradienty funkcji celu liczonych na kolejno przetwarzanych instancjach lub przy treningu modelu z wieloma funkcjami celu; tutaj jednak doprowadziło to do błędnego wyniku. Z tego powodu bardzo ważne jest pamiętanie o wyzerowaniu wartości gradientów przed przystąpieniem do kolejnych obliczeń.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1429, 0.2857, 0.4286])\n"
     ]
    }
   ],
   "source": [
    "result = (x**2).sum()\n",
    "result2 = torch.log(result)\n",
    "result2.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwróć uwagę na konwencję biblioteki PyTorch - jeśli nazwa funkcji zakończona jest podkreślnikiem to taka operacja jest wykonywana `in-place`. (np. `x.add(5)` - `x` nadal ma stałą wartość, `x.add_(5)` wartość `x` zwiększono o 5).\n",
    "\n",
    "Zaimplementujmy podstawowy algorytm uczący w PyTorch. Będzie to prosta sieć neuronowa składająca się z:\n",
    "- macierzy zanurzeń $C$, przetwarzającej indeksy słów na odpowiednie reprezentacje wektorowe, \n",
    "- operacji uśredniania tych zanurzeń do jednego zanurzenia (average pooling over time) \n",
    "- oraz jednej warstwy liniowej (softmax) zwracającej wynik.\n",
    "\n",
    "Algorytmem uczącym będzie SGD optymalizujące entropię krzyżową, czyli dla kolejnych instancji uczących będziemy wykonywać:\n",
    "$$parametry = parametry - \\eta \\nabla f\\_celu$$\n",
    "Implementacja ta będzie wyjątkowo prosta, gdyż gradient funkcji celu ($\\nabla f\\_celu$) zostanie obliczony automatycznie przez PyTorch. Ponadto entropia krzyżowa jest już zaimplementowana w PyTorch `F.cross_entropy(logits, target)`. Zwróć uwagę, że argumentem tej funkcji są wartości logitów (nie trzeba implementować funkcji softmax przetwarzającej wartości logitów na prawdopodobieństwa).\n",
    "\n",
    "**UWAGA** W implementacji nie należy używać gotowych implementacji SGD czy warstw sieci neuronowych w PyTorch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pierwszym krokiem w implementacji będzie zaimplementowanie samego modelu. Należy zainicjalizować macierz $C$ przechowującą w wierszach zanurzenia dla kolejnych słów (liczba słów to `n_words`, wymiarowość zanurzenia określ na 20) oraz macierz $W$ przechowującą parametry warstwy liniowej, zwracającej wartości logitów dla każdej z klas (liczba klas to `n_class`). Macierz $W$ w dodatkowej kolumnie powinna też przechowywać wartości wyrazów wolnych (bias). Wartości zainicjalizuj losowo `torch.rand`. Pamiętaj, że dla tych macierzy będziesz potrzebował wyznaczyć potem wartości gradientów.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "134cf7cd8660133b178caff1ca2fcdfc",
     "grade": false,
     "grade_id": "cell-9ba79ce47d0c6a5e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 20\n",
    "\n",
    "def initialize_tensors(size):\n",
    "    C = torch.rand((n_words, size), requires_grad=True)\n",
    "    W = torch.rand((n_class, size + 1), requires_grad=True)\n",
    "    return C, W\n",
    "\n",
    "C, W = initialize_tensors(EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "084514b8fbf6ffa4f784b5b398bd3624",
     "grade": true,
     "grade_id": "cell-b3436825d33d5941",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimplementuj funkcję `simple_model`, której argumentem będzie instancja testowa (jest to więc lista indeksów słów występujących w tekście), a na której wyjściu będzie wektor `n_class`-elementowy zawierający obliczone wartości logitów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "280f8962c6b7f20c718a51b6ecc294a4",
     "grade": false,
     "grade_id": "cell-e2668a9abacbc78a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def simple_model(x):\n",
    "    doc_embedding = torch.mean(C[x], dim=0)\n",
    "    doc_with_bias = torch.cat([torch.ones(1),doc_embedding]) # Skonkatenowanie 1 z uzyskaną reprezentacją (bias)\n",
    "    return W @ doc_with_bias # Obliczenie wartości logitów (tj. warstwa liniowa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimplementuj algorytm SGD w poniższej pętli. Pętla ta iteruje po zbiorze uczącym oraz dla każdej instancji oblicza wartość funkcji celu. Twoje zadania:\n",
    "- Policz gradienty (faza *backward*)\n",
    "- Zaimplementuj aktualizacje $W$ i $C$ wg. wzoru na SGD. Operacje modyfikujące $W$ i $C$ musisz wykonać w środku klauzuli `with torch.no_grad():`, aby nie śledzić z tych operacji gradientów.\n",
    "- Pamiętaj o wyczyszczeniu gradientów (zarówno w $W$ jak i $C$)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80acc452845b941b086f5aa582f26d87",
     "grade": false,
     "grade_id": "cell-aeedd0f5183bd781",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: avg. train loss=1.1355\n",
      "iter 1: avg. train loss=0.9601\n",
      "iter 2: avg. train loss=0.9460\n",
      "iter 3: avg. train loss=0.9054\n",
      "iter 4: avg. train loss=0.8783\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "epochs = 5\n",
    "eta = 0.5  # prędkość uczenia\n",
    "C, W = initialize_tensors(EMBEDDING_SIZE)\n",
    "\n",
    "for i in range(epochs):\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train:\n",
    "        pred = simple_model(words)\n",
    "        loss = F.cross_entropy(pred.reshape(1,-1), torch.tensor(tag).reshape(1))\n",
    "        # Loss zawiera wartość funkcji celu dla przykładu, wykonaj backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            train_loss += loss\n",
    "\n",
    "            W -= eta * W.grad\n",
    "            C -= eta * C.grad\n",
    "\n",
    "            W.grad.zero_()\n",
    "            C.grad.zero_()\n",
    "    print(\"iter %r: avg. train loss=%.4f\" % (i, train_loss / len(train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenia**\n",
    "- Dlaczego w bibliotekach do głębokiego uczenia maszynowego, takich jak PyTorch, implementuje się funkcje entropii krzyżowej tak, aby przyjmowała na wejście wartości logitów zamiast prawdopodobieństw z softmax?\n",
    "- Na wykładzie pokazywaliśmy warstwę zanurzeń jako warstwę mnożącą macierz $C$ przez wektor \"1 z n\", można ją jednak także zaimplementować jako operację odczytu odpowiedniego wiersza z macierzy. Jakie są wady i zalety obu tych sposobów implementacji?\n",
    "\n",
    "Odpowiedź na pierwszą kropkę umieść poniżej.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5158398dc380a8a01088a617375d8f4c",
     "grade": true,
     "grade_id": "cell-c685493684bda3e3",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "WPISZ TWOJĄ ODPOWIEDŹ TUTAJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 3 - wykorzystanie nn.Module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch jako biblioteka do głębokiego uczenia maszynowego oferuje nam kilka udogodnień w implementowaniu modeli uczących się, aby jeszcze bardziej uprościć ich implementację. Większość z tych udogodnień związanych z jest z reprezentowaniem modeli uczących się jako obiektów dziedziczących po `torch.nn.Module`. W obiekcie takim powinniśmy zaimplementować co najmniej konstruktor, inicjalizujący parametry modelu ($W$ i $C$), oraz funkcję `forward` obliczającą wynik modelu (we wcześniejszym zadaniu nazywaliśmy ją `simple_model`). Ponadto moduł `torch.nn` oferuje gotowe implementacje zarówno warstwy liniowej jak i warstwy zanurzeń.\n",
    "\n",
    "Przeanalizuj poniższą implementację modelu z poprzedniego zadania.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self, n_words, emb_size, n_class):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(n_words, emb_size)\n",
    "        self.linear = torch.nn.Linear(in_features=emb_size, out_features=n_class, bias=True)\n",
    "        torch.nn.init.uniform_(self.embedding.weight, -0.25, 0.25)\n",
    "        torch.nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "    def forward(self, words):\n",
    "        emb = self.embedding(words)                 \n",
    "        h = emb.mean(dim=0)                         \n",
    "        h = torch.reshape(h, (1,-1))\n",
    "        out = self.linear(h)              \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oprócz tego, że uzyskaliśmy elegancki obiekt reprezentujący nasz model, nie wydaje się by powyższa implementacja była krótsza czy prostsza od tej, którą uzyskaliśmy w poprzednim zadaniu bez dobrodziejstw `nn.Module`. Co zatem zyskaliśmy?\n",
    "\n",
    "Przy implementacji modeli z dużą liczbą warstw, szczególnie uciążliwe byłoby implementowanie kolejnych linijek kodu zerujących gradienty wszystkich macierzy wag, oraz wykonywanie na nich kroków algorytmu SGD. W naszej implementacji każda macierz parametrów to dwie linijki kodu! Jednak modele dziedziczące po `torch.nn.Module` i stworzone poprzez dedykowane warstwy neuronowe posiadają gotową funkcję `parameters()` zwracającą kolejne macierze parametrów modelu.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.1609,  0.0989, -0.0518,  ..., -0.2365,  0.0248, -0.0152],\n",
      "        [-0.2254,  0.0394, -0.0669,  ..., -0.1842,  0.0470,  0.2063],\n",
      "        [-0.1310, -0.0271, -0.2397,  ..., -0.1859, -0.1206, -0.2478],\n",
      "        ...,\n",
      "        [-0.0933,  0.1210,  0.1976,  ..., -0.2445, -0.0927, -0.1077],\n",
      "        [ 0.1012,  0.0211, -0.1043,  ...,  0.1180,  0.1067, -0.1309],\n",
      "        [-0.0363,  0.2183,  0.1583,  ...,  0.0137, -0.0222,  0.1815]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.1161,  0.0693,  0.2821, -0.2650, -0.4008, -0.2819, -0.1938,  0.2585,\n",
      "          0.2546,  0.4209,  0.4725, -0.1389, -0.1003,  0.1012,  0.1961,  0.2380,\n",
      "          0.4339, -0.1890,  0.2815, -0.3693],\n",
      "        [-0.3983,  0.3975,  0.3072,  0.2348,  0.3773, -0.1205,  0.2124, -0.2139,\n",
      "          0.1748, -0.3769, -0.4518,  0.1269,  0.3118, -0.2520, -0.1354, -0.0948,\n",
      "          0.1195, -0.1540,  0.0308, -0.0587],\n",
      "        [-0.4726,  0.0852,  0.2667, -0.4116,  0.4858, -0.4603,  0.2669,  0.4756,\n",
      "         -0.1615,  0.1337,  0.0589,  0.2866,  0.2598, -0.3898, -0.4848, -0.2347,\n",
      "         -0.3863, -0.0504, -0.4543,  0.1994],\n",
      "        [-0.2829, -0.0936, -0.4322, -0.2831, -0.2841,  0.1941,  0.2832, -0.1785,\n",
      "          0.2880, -0.2941, -0.3678,  0.3753, -0.3664, -0.3654,  0.2500, -0.4663,\n",
      "         -0.4165,  0.0832, -0.1006, -0.1162],\n",
      "        [ 0.0133, -0.2490,  0.4094, -0.0874,  0.3041,  0.4342,  0.0102, -0.0008,\n",
      "         -0.2674, -0.2596,  0.2104,  0.0400, -0.4030, -0.3919,  0.1294,  0.1383,\n",
      "         -0.4597, -0.1735,  0.1962,  0.2224]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.1066, -0.0488, -0.1328, -0.1496,  0.2069], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "model = SimpleModel(n_words, EMBEDDING_SIZE, n_class)\n",
    "print([i for i in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jest to niezwykle wygodne, bo implementacja algorytmu SGD może przeiterować po tej liście parametrów i dla każdej z nich wykonać aktualizację ich wartości. Fakt, że taka lista jest tworzona automatycznie pozbawia nas ryzyka, że zwyczajnie o którejś macierzy parametrów czy wektorze wyrazów wolnych najzwyczajniej zapomnimy. \n",
    "\n",
    "Podobnie można zaimplementować pętlę zerującą gradienty wszystkich parametrów. Modele oferują nawet gotową taką funkcję `model.zero_grad()`, która iteruje po parametrach zerując ich gradienty. \n",
    "\n",
    "Zmodyfikuj implementację SGD z poprzedniego zadania, tak aby wykorzystywała `zero_grad()` i `parameters()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5f2f3a6cb7d7b2d2e34e63a2b54004b",
     "grade": false,
     "grade_id": "cell-87d2dfc0c801725a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter Parameter containing:\n",
      "tensor([ 1.9933,  0.4218,  1.2478, -1.7495, -1.9941], requires_grad=True): avg. train loss=0.9863\n",
      "iter Parameter containing:\n",
      "tensor([ 2.5760, -0.3060,  1.5082, -1.6670, -2.1920], requires_grad=True): avg. train loss=0.9618\n",
      "iter Parameter containing:\n",
      "tensor([ 2.7519, -0.4110,  1.4690, -1.7125, -2.1782], requires_grad=True): avg. train loss=0.9248\n",
      "iter Parameter containing:\n",
      "tensor([ 2.5992, -1.0454,  1.1576, -1.6292, -1.1630], requires_grad=True): avg. train loss=0.8918\n",
      "iter Parameter containing:\n",
      "tensor([ 1.6563, -0.1511,  1.8729, -1.4825, -1.9764], requires_grad=True): avg. train loss=0.8778\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "eta = 0.5  # prędkość uczenia\n",
    "\n",
    "for i in range(epochs):\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train:\n",
    "        pred = model(torch.tensor(words))\n",
    "        loss = F.cross_entropy(pred.reshape(1,-1), torch.tensor(tag).reshape(1))\n",
    "        # Loss zawiera wartość funkcji celu dla przykładu, wykonaj backpropagation\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            train_loss += loss\n",
    "\n",
    "            for i in model.parameters():\n",
    "                i -= eta * i.grad\n",
    "\n",
    "            model.zero_grad()\n",
    "        \n",
    "    print(\"iter %r: avg. train loss=%.4f\" % (i, train_loss / len(train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodatkowo moduł `torch.nn` oferuje także od razu zaimplementowane optymalizatory, w tym SGD. W konstruktorze optymalizatora należy podać listę optymalizowanych przez niego parametrów, a następnie wywołać na nim procedurę `step()` wykonującą krok algorytmu optymalizacyjnego tj. aktualizację wartości zmiennych przy użyciu gradientu. W tej sytuacji nie musisz się martwić o umieszczanie kodu zmieniającego parametry w `with torch.no_grad()` - optymalizator sam to zrobi! Optymalizator również oferuje funkcję `zero_grad()`, zerującą gradienty zmiennych wskazanych do optymalizacji.\n",
    "\n",
    "Zmodyfikuj kod z poprzedniego zadania, tak aby wykorzystywał optymalizator SGD zaimplementowany w `torch.optim`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92b92a0626a6c533b3b61aa2e6c18ddd",
     "grade": false,
     "grade_id": "cell-5588a40beb11d256",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: avg. train loss=0.5481\n",
      "iter 1: avg. train loss=0.5378\n",
      "iter 2: avg. train loss=0.4791\n",
      "iter 3: avg. train loss=0.4586\n",
      "iter 4: avg. train loss=0.4480\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "eta = 0.5  # prędkość uczenia\n",
    "\n",
    "for i in range(epochs):\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=eta)\n",
    "    for words, tag in train:\n",
    "        pred = model(torch.tensor(words))\n",
    "        loss = F.cross_entropy(pred.reshape(1,-1), torch.tensor(tag).reshape(1))\n",
    "        train_loss += loss\n",
    "        # Loss zawiera wartość funkcji celu dla przykładu, wykonaj backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        \n",
    "    print(\"iter %r: avg. train loss=%.4f\" % (i, train_loss / len(train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenia**\n",
    "- Przeanalizuj dokładnie powyższy kod i przechodząc linia po linii, wyjaśnij co one robią z punktu widzenia treningu modelu.\n",
    "- Zastanów się jak wyglądałaby Twoja własna implementacja klasy `optim.SGD`.\n",
    "- Prześledź jeszcze raz implementację modelu neuronowego - pewnie w niedługim czasie będziesz implementował znacznie bardziej skomplikowane modele, tym bardziej warto je dobrze prześledzić!\n",
    "- Czym różni się zaimplementowana architektura od głębokiej sieci uśredniającej?\n",
    "- Na wykładzie korzystaliśmy z macierzy zanurzeń w modelach języka. Tutaj warstwa zanurzeń pojawiła się bezpośrednio w modelu klasyfikacji. Czy w uzyskanych w ten sposób zanurzeniach (zakładając dobry dobór hiperparamerów, dodanie regularyzacji itd.) zaobserwowalibyśmy podobne zależności jak te uzyskane za pomocą modelu języka? Jeśli nie, obserwacji jakich zależności między słowami spodziewałbyś się w tej reprezentacji? Skąd biorą się różnice?\n",
    "\n",
    "Odpowiedź na ostatnią kropkę umieść poniżej.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "731e0439a761d52c3124060a972eaf31",
     "grade": true,
     "grade_id": "cell-50c2ecade40bcd7f",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "WPISZ TWOJĄ ODPOWIEDŹ TUTAJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "73080ad0900acee85ddd2dd6ff52a07d",
     "grade": false,
     "grade_id": "cell-a81ef968001b7310",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Zadanie 4\n",
    "Wykorzystując wiedzę z poprzedniego zadania zaimplementuj prostą architekturę splotową do klasyfikacji tekstu i wytrenuj ją. Do jej wykonania może być przydatna klasa `torch.nn.Conv1d` i funkcja `torch.nn.ReLU` (zapoznaj się z ich dokumentacją w Internecie). Jako funkcji redukcji użyj funkcji maksimum (over time).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8e23fa15d0164d9a5260ab5ea518263",
     "grade": false,
     "grade_id": "cell-dbb714252ae2ebc7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: avg. train loss=1.1457\n",
      "iter 1: avg. train loss=0.9828\n",
      "iter 2: avg. train loss=0.9686\n",
      "iter 3: avg. train loss=0.9581\n",
      "iter 4: avg. train loss=0.9422\n",
      "iter 5: avg. train loss=0.9588\n",
      "iter 6: avg. train loss=0.9332\n",
      "iter 7: avg. train loss=0.9199\n",
      "iter 8: avg. train loss=0.9176\n",
      "iter 9: avg. train loss=0.9300\n",
      "iter 10: avg. train loss=0.8946\n",
      "iter 11: avg. train loss=0.9203\n",
      "iter 12: avg. train loss=0.9072\n",
      "iter 13: avg. train loss=0.8523\n",
      "iter 14: avg. train loss=0.8681\n",
      "iter 15: avg. train loss=0.8774\n",
      "iter 16: avg. train loss=0.8103\n",
      "iter 17: avg. train loss=0.7365\n",
      "iter 18: avg. train loss=0.8391\n",
      "iter 19: avg. train loss=0.7542\n"
     ]
    }
   ],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, n_words, emb_size, num_filters, window_size, ntags):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(n_words, emb_size)\n",
    "        self.conv_layer = torch.nn.Conv1d(in_channels=emb_size, out_channels=num_filters, kernel_size=window_size)\n",
    "        self.linear = torch.nn.Linear(in_features=num_filters, out_features=ntags, bias=True)\n",
    "\n",
    "    def kmax_pooling(self, x, dim, k):\n",
    "        index = x.topk(k, dim = dim)[1].sort(dim = dim)[0]\n",
    "        return x.gather(dim, index)\n",
    "\n",
    "    def forward(self, words):\n",
    "        emb = self.embedding(words)\n",
    "        emb = emb.permute(1,0)\n",
    "        conv_output = self.conv_layer(emb)\n",
    "        relu = F.relu(conv_output)\n",
    "        kmax = self.kmax_pooling(relu, 1, 1).squeeze()\n",
    "        out = self.linear(kmax)\n",
    "        return out\n",
    "        \n",
    "model = CNN(n_words=n_words, emb_size=EMBEDDING_SIZE, num_filters=64, window_size=2, ntags=n_class)\n",
    "\n",
    "epochs = 20\n",
    "eta = 0.05 \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=eta)\n",
    "\n",
    "for i in range(epochs):\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train:\n",
    "        pred = model(torch.tensor(words))\n",
    "        loss = F.cross_entropy(pred.reshape(1,-1), torch.tensor(tag).reshape(1))\n",
    "        loss.backward()\n",
    "        train_loss += loss\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(\"iter %r: avg. train loss=%.4f\" % (i, train_loss / len(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
